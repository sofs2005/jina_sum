# encoding:utf-8
import json
import os
import html
import re
from urllib.parse import urlparse, quote
import time

import requests
from newspaper import Article
import newspaper
from bs4 import BeautifulSoup

import plugins
from bridge.context import ContextType
from bridge.reply import Reply, ReplyType
from common.log import logger
from plugins import *

@plugins.register(
    name="JinaSum",
    desire_priority=20,
    hidden=False,
    desc="Sum url link content with newspaper3k and llm",
    version="2.2",
    author="sofs2005",
)
class JinaSum(Plugin):
    """ç½‘é¡µå†…å®¹æ€»ç»“æ’ä»¶
    
    åŠŸèƒ½ï¼š
    1. è‡ªåŠ¨æ€»ç»“åˆ†äº«çš„ç½‘é¡µå†…å®¹
    2. æ”¯æŒæ‰‹åŠ¨è§¦å‘æ€»ç»“
    3. æ”¯æŒç¾¤èŠå’Œå•èŠä¸åŒå¤„ç†æ–¹å¼
    4. æ”¯æŒé»‘åå•ç¾¤ç»„é…ç½®
    """
    # é»˜è®¤é…ç½®
    DEFAULT_CONFIG = {
        "max_words": 8000,
        "prompt": "æˆ‘éœ€è¦å¯¹ä¸‹é¢å¼•å·å†…æ–‡æ¡£è¿›è¡Œæ€»ç»“ï¼Œæ€»ç»“è¾“å‡ºåŒ…æ‹¬ä»¥ä¸‹ä¸‰ä¸ªéƒ¨åˆ†ï¼š\nğŸ“– ä¸€å¥è¯æ€»ç»“\nğŸ”‘ å…³é”®è¦ç‚¹,ç”¨æ•°å­—åºå·åˆ—å‡º3-5ä¸ªæ–‡ç« çš„æ ¸å¿ƒå†…å®¹\nğŸ· æ ‡ç­¾: #xx #xx\nè¯·ä½¿ç”¨emojiè®©ä½ çš„è¡¨è¾¾æ›´ç”ŸåŠ¨\n\n",
        "white_url_list": [],
        "black_url_list": [
            "https://support.weixin.qq.com",  # è§†é¢‘å·è§†é¢‘
            "https://channels-aladin.wxqcloud.qq.com",  # è§†é¢‘å·éŸ³ä¹
        ],
        "black_group_list": [],
        "auto_sum": True,
        "cache_timeout": 300,  # ç¼“å­˜è¶…æ—¶æ—¶é—´ï¼ˆ5åˆ†é’Ÿï¼‰
    }

    def __init__(self):
        """åˆå§‹åŒ–æ’ä»¶é…ç½®"""
        try:
            super().__init__()
            
            # ç¡®ä¿ä½¿ç”¨é»˜è®¤é…ç½®åˆå§‹åŒ–
            self.config = super().load_config()
            if not self.config:
                self.config = self._load_config_template()
            
            # ä½¿ç”¨é»˜è®¤é…ç½®åˆå§‹åŒ–
            for key, default_value in self.DEFAULT_CONFIG.items():
                if key not in self.config:
                    self.config[key] = default_value
            
            # è®¾ç½®é…ç½®å‚æ•°
            self.max_words = self.config.get("max_words", 8000)
            self.prompt = self.config.get("prompt", "æˆ‘éœ€è¦å¯¹ä¸‹é¢å¼•å·å†…æ–‡æ¡£è¿›è¡Œæ€»ç»“...")
            self.cache_timeout = self.config.get("cache_timeout", 300)  # é»˜è®¤5åˆ†é’Ÿ
            
            # URLé»‘ç™½åå•é…ç½®
            self.white_url_list = self.config.get("white_url_list", [])
            self.black_url_list = self.config.get("black_url_list", [])
            self.black_group_list = self.config.get("black_group_list", [])
            
            # æ˜¯å¦è‡ªåŠ¨æ€»ç»“ï¼ˆä»…ç¾¤èŠæœ‰æ•ˆï¼‰
            self.auto_sum = self.config.get("auto_sum", False)
            
            # æ¶ˆæ¯ç¼“å­˜
            self.pending_messages = {}  # ç”¨äºå­˜å‚¨å¾…å¤„ç†çš„æ¶ˆæ¯ï¼Œæ ¼å¼: {chat_id: {"content": content, "timestamp": time.time()}}
            
            # API è®¾ç½®
            self.open_ai_api_base = "https://api.openai.com/v1"
            self.open_ai_model = "gpt-3.5-turbo"
            
            logger.info(f"[JinaSum] åˆå§‹åŒ–å®Œæˆ, config={self.config}")
            self.handlers[Event.ON_HANDLE_CONTEXT] = self.on_handle_context
        except Exception as e:
            logger.error(f"[JinaSum] åˆå§‹åŒ–å¼‚å¸¸ï¼š{str(e)}", exc_info=True)
            raise Exception("[JinaSum] åˆå§‹åŒ–å¤±è´¥")

    def on_handle_context(self, e_context: EventContext):
        """å¤„ç†æ¶ˆæ¯"""
        context = e_context['context']
        logger.info(f"[JinaSum] æ”¶åˆ°æ¶ˆæ¯, ç±»å‹={context.type}, å†…å®¹é•¿åº¦={len(context.content)}")

        # é¦–å…ˆåœ¨æ—¥å¿—ä¸­è®°å½•å®Œæ•´çš„æ¶ˆæ¯å†…å®¹ï¼Œä¾¿äºè°ƒè¯•
        orig_content = context.content
        if len(orig_content) > 500:
            logger.info(f"[JinaSum] æ¶ˆæ¯å†…å®¹(æˆªæ–­): {orig_content[:500]}...")
        else:
            logger.info(f"[JinaSum] æ¶ˆæ¯å†…å®¹: {orig_content}")
        
        if context.type not in [ContextType.TEXT, ContextType.SHARING]:
            logger.info(f"[JinaSum] æ¶ˆæ¯ç±»å‹ä¸ç¬¦åˆå¤„ç†æ¡ä»¶ï¼Œè·³è¿‡: {context.type}")
            return

        content = context.content
        channel = e_context['channel']
        msg = e_context['context']['msg']
        chat_id = msg.from_user_id
        is_group = msg.is_group
        
        # æ‰“å°å‰50ä¸ªå­—ç¬¦ç”¨äºè°ƒè¯•
        preview = content[:50] + "..." if len(content) > 50 else content
        logger.info(f"[JinaSum] å¤„ç†æ¶ˆæ¯: {preview}, ç±»å‹={context.type}")

        # æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸ºXMLæ ¼å¼ï¼ˆå“”å“©å“”å“©ç­‰ç¬¬ä¸‰æ–¹åˆ†äº«å¡ç‰‡ï¼‰
        if content.startswith('<?xml') or (content.startswith('<msg>') and '<appmsg' in content) or ('<appmsg' in content and '<url>' in content):
            logger.info("[JinaSum] æ£€æµ‹åˆ°XMLæ ¼å¼åˆ†äº«å¡ç‰‡ï¼Œå°è¯•æå–URL")
            try:
                import xml.etree.ElementTree as ET
                # å¤„ç†å¯èƒ½çš„XMLå£°æ˜
                if content.startswith('<?xml'):
                    content = content[content.find('<msg>'):]
                
                # å¦‚æœä¸æ˜¯å®Œæ•´çš„XMLï¼Œå°è¯•æ·»åŠ æ ¹èŠ‚ç‚¹
                if not content.startswith('<msg') and '<appmsg' in content:
                    content = f"<msg>{content}</msg>"
                
                # å¯¹äºä¸€äº›å¯èƒ½æ ¼å¼ä¸æ ‡å‡†çš„XMLï¼Œä½¿ç”¨æ›´å®½æ¾çš„è§£ææ–¹å¼
                try:
                    root = ET.fromstring(content)
                except ET.ParseError:
                    # å°è¯•ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–URL
                    import re
                    url_match = re.search(r'<url>(.*?)</url>', content)
                    if url_match:
                        extracted_url = url_match.group(1)
                        logger.info(f"[JinaSum] é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼ä»XMLä¸­æå–åˆ°URL: {extracted_url}")
                        content = extracted_url
                        context.type = ContextType.SHARING
                        context.content = extracted_url
                    else:
                        logger.error("[JinaSum] æ— æ³•é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼ä»XMLä¸­æå–URL")
                        return
                else:
                    # XMLè§£ææˆåŠŸ
                    url_elem = root.find('.//url')
                    title_elem = root.find('.//title')
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰appinfoèŠ‚ç‚¹ï¼Œåˆ¤æ–­æ˜¯å¦ä¸ºBç«™ç­‰ç‰¹æ®Šåº”ç”¨
                    appinfo = root.find('.//appinfo')
                    app_name = None
                    if appinfo is not None and appinfo.find('appname') is not None:
                        app_name = appinfo.find('appname').text
                        logger.info(f"[JinaSum] æ£€æµ‹åˆ°APPåˆ†äº«: {app_name}")
                    
                    logger.info(f"[JinaSum] XMLè§£æç»“æœ: url_elem={url_elem is not None}, title_elem={title_elem is not None}, app_name={app_name}")
                    
                    if url_elem is not None and url_elem.text:
                        # æå–åˆ°URLï¼Œå°†ç±»å‹ä¿®æ”¹ä¸ºSHARING
                        extracted_url = url_elem.text
                        logger.info(f"[JinaSum] ä»XMLä¸­æå–åˆ°URL: {extracted_url}")
                        content = extracted_url
                        context.type = ContextType.SHARING
                        context.content = extracted_url
                        
                        # å¯¹äºBç«™è§†é¢‘é“¾æ¥ï¼Œè®°å½•é¢å¤–ä¿¡æ¯
                        if app_name and ("å“”å“©å“”å“©" in app_name or "bilibili" in app_name.lower() or "bç«™" in app_name):
                            logger.info("[JinaSum] æ£€æµ‹åˆ°Bç«™è§†é¢‘åˆ†äº«")
                            # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ Bç«™è§†é¢‘çš„ç‰¹æ®Šå¤„ç†é€»è¾‘
                    else:
                        logger.error("[JinaSum] æ— æ³•ä»XMLä¸­æå–URL")
                        return
            except Exception as e:
                logger.error(f"[JinaSum] è§£æXMLå¤±è´¥: {str(e)}", exc_info=True)
                return

        # æ£€æŸ¥æ˜¯å¦éœ€è¦è‡ªåŠ¨æ€»ç»“
        should_auto_sum = self.auto_sum
        if should_auto_sum and is_group and msg.from_user_nickname in self.black_group_list:
            should_auto_sum = False

        # æ¸…ç†è¿‡æœŸç¼“å­˜
        self._clean_expired_cache()

        # å¤„ç†åˆ†äº«æ¶ˆæ¯
        if context.type == ContextType.SHARING:
            logger.debug("[JinaSum] Processing SHARING message")
            if is_group:
                if should_auto_sum:
                    return self._process_summary(content, e_context, retry_count=0)
                else:
                    self.pending_messages[chat_id] = {
                        "content": content,
                        "timestamp": time.time()
                    }
                    logger.debug(f"[JinaSum] Cached SHARING message: {content}, chat_id={chat_id}")
                    return
            else:  # å•èŠæ¶ˆæ¯ç›´æ¥å¤„ç†
                return self._process_summary(content, e_context, retry_count=0)

        # å¤„ç†æ–‡æœ¬æ¶ˆæ¯
        elif context.type == ContextType.TEXT:
            logger.debug("[JinaSum] Processing TEXT message")
            content = content.strip()
            
            # ç§»é™¤å¯èƒ½çš„@ä¿¡æ¯
            if content.startswith("@"):
                parts = content.split(" ", 1)
                if len(parts) > 1:
                    content = parts[1].strip()
                else:
                    content = ""
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«"æ€»ç»“"å…³é”®è¯ï¼ˆä»…ç¾¤èŠéœ€è¦ï¼‰
            if is_group and "æ€»ç»“" in content:
                logger.debug(f"[JinaSum] Found summary trigger, pending_messages={self.pending_messages}")
                if chat_id in self.pending_messages:
                    cached_content = self.pending_messages[chat_id]["content"]
                    logger.debug(f"[JinaSum] Processing cached content: {cached_content}")
                    del self.pending_messages[chat_id]
                    return self._process_summary(cached_content, e_context, retry_count=0, skip_notice=False)
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯ç›´æ¥URLæ€»ç»“ï¼Œç§»é™¤"æ€»ç»“"å¹¶æ£€æŸ¥å‰©ä½™å†…å®¹æ˜¯å¦ä¸ºURL
                url = content.replace("æ€»ç»“", "").strip()
                if url and self._check_url(url):
                    logger.debug(f"[JinaSum] Processing direct URL: {url}")
                    return self._process_summary(url, e_context, retry_count=0, skip_notice=False)
                logger.debug("[JinaSum] No content to summarize")
                return

            # å¤„ç†"é—®xxx"æ ¼å¼çš„è¿½é—®
            if content.startswith("é—®"):
                question = content[1:].strip()
                if question:
                    logger.debug(f"[JinaSum] Processing question: {question}")
                    return self._process_question(question, chat_id, e_context)
                else:
                    logger.debug("[JinaSum] Empty question, ignored")
                    return
                    
            # å•èŠä¸­ç›´æ¥å¤„ç†URL
            if not is_group and self._check_url(content):
                return self._process_summary(content, e_context, retry_count=0)

    def _clean_expired_cache(self):
        """æ¸…ç†è¿‡æœŸçš„ç¼“å­˜"""
        current_time = time.time()
        # æ¸…ç†å¾…å¤„ç†æ¶ˆæ¯ç¼“å­˜
        expired_keys = [
            k for k, v in self.pending_messages.items() 
            if current_time - v["timestamp"] > self.cache_timeout
        ]
        for k in expired_keys:
            del self.pending_messages[k]

    def _get_content_via_api(self, url):
        """é€šè¿‡APIæœåŠ¡è·å–å¾®ä¿¡å…¬ä¼—å·å†…å®¹
        
        å½“jinaç›´æ¥è®¿é—®å¤±è´¥æ—¶ï¼Œä½¿ç”¨æ­¤å¤‡ç”¨æ–¹æ³•
        
        Args:
            url: å¾®ä¿¡æ–‡ç« URL
            
        Returns:
            str: æ–‡ç« å†…å®¹
        """
        try:
            # ç®€å•çš„APIè°ƒç”¨ï¼Œå‚è€ƒsum4allæ’ä»¶å®ç°
            api_url = "https://ai.sum4all.site"
            headers = {
                'Content-Type': 'application/json'
            }
            payload = {
                "link": url,
                "prompt": "",  # ä¸éœ€è¦æ€»ç»“ï¼Œåªè·å–å†…å®¹
            }
            
            logger.debug(f"[JinaSum] Trying to get content via API: {url}")
            response = requests.post(api_url, headers=headers, json=payload)
            response.raise_for_status()
            
            response_data = response.json()
            if response_data.get("success"):
                # ä»APIè¿”å›ä¸­æå–åŸå§‹å†…å®¹
                content = response_data.get("content", "")
                if content:
                    logger.debug(f"[JinaSum] Successfully got content via API, length: {len(content)}")
                    return content
            
            logger.error(f"[JinaSum] API returned failure or empty content")
            return None
        except Exception as e:
            logger.error(f"[JinaSum] Error getting content via API: {str(e)}")
            return None

    def _get_content_via_newspaper(self, url):
        """ä½¿ç”¨newspaper3kåº“æå–æ–‡ç« å†…å®¹
        
        Args:
            url: æ–‡ç« URL
            
        Returns:
            str: æ–‡ç« å†…å®¹,å¤±è´¥è¿”å›None
        """
        try:
            # å¤„ç†Bç«™çŸ­é“¾æ¥
            if "b23.tv" in url:
                # å…ˆè·å–é‡å®šå‘åçš„çœŸå®URL
                try:
                    logger.debug(f"[JinaSum] Resolving Bç«™çŸ­é“¾æ¥: {url}")
                    headers = {
                        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
                        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
                        "Cache-Control": "max-age=0",
                        "Connection": "keep-alive"
                    }
                    response = requests.head(url, headers=headers, allow_redirects=True, timeout=10)
                    if response.status_code == 200:
                        real_url = response.url
                        logger.debug(f"[JinaSum] Bç«™çŸ­é“¾æ¥è§£æç»“æœ: {real_url}")
                        url = real_url
                except Exception as e:
                    logger.error(f"[JinaSum] è§£æBç«™çŸ­é“¾æ¥å¤±è´¥: {str(e)}")
            
            # å¢å¼ºæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨è®¿é—®
            import random
            
            # éšæœºé€‰æ‹©ä¸€ä¸ªUser-Agentï¼Œæ¨¡æ‹Ÿä¸åŒæµè§ˆå™¨
            user_agents = [
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0",
                "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.5 Safari/605.1.15",
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0",
                "Mozilla/5.0 (iPhone; CPU iPhone OS 17_0_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1"
            ]
            selected_ua = random.choice(user_agents)
            
            # æ„å»ºæ›´çœŸå®çš„è¯·æ±‚å¤´
            headers = {
                "User-Agent": selected_ua,
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
                "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
                "Accept-Encoding": "gzip, deflate, br",
                "Connection": "keep-alive",
                "Upgrade-Insecure-Requests": "1",
                "Sec-Fetch-Dest": "document",
                "Sec-Fetch-Mode": "navigate",
                "Sec-Fetch-Site": "none",
                "Sec-Fetch-User": "?1",
                "Cache-Control": "max-age=0"
            }
            
            # è®¾ç½®ä¸€ä¸ªéšæœºçš„å¼•èæ¥æºï¼Œå¾®ä¿¡æ–‡ç« æœ‰æ—¶éœ€è¦Referer
            referers = [
                "https://www.baidu.com/",
                "https://www.google.com/",
                "https://www.bing.com/",
                "https://mp.weixin.qq.com/",
                "https://weixin.qq.com/",
                "https://www.qq.com/"
            ]
            if random.random() > 0.3:  # 70%çš„æ¦‚ç‡æ·»åŠ Referer
                headers["Referer"] = random.choice(referers)
                
            # ä¸ºå¾®ä¿¡å…¬ä¼—å·æ–‡ç« æ·»åŠ ç‰¹æ®Šå¤„ç†
            if "mp.weixin.qq.com" in url:
                try:
                    # æ·»åŠ å¿…è¦çš„å¾®ä¿¡Cookieå‚æ•°ï¼Œå‡å°‘è¢«æ£€æµ‹çš„å¯èƒ½æ€§
                    cookies = {
                        "appmsglist_action_3941382959": "card",  # ä¸€äº›éšæœºçš„Cookieå€¼
                        "appmsglist_action_3941382968": "card",
                        "pac_uid": f"{int(time.time())}_f{random.randint(10000, 99999)}",
                        "rewardsn": "",
                        "wxtokenkey": f"{random.randint(100000, 999999)}",
                    }
                    
                    # ç›´æ¥ä½¿ç”¨requestsè¿›è¡Œå†…å®¹è·å–ï¼Œæœ‰æ—¶æ¯”newspaperæ›´æœ‰æ•ˆ
                    session = requests.Session()
                    response = session.get(url, headers=headers, cookies=cookies, timeout=20)
                    response.raise_for_status()
                    
                    # ä½¿ç”¨BeautifulSoupç›´æ¥è§£æ
                    from bs4 import BeautifulSoup
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # å¾®ä¿¡æ–‡ç« é€šå¸¸æœ‰è¿™äº›ç‰¹å¾
                    title_elem = soup.select_one('#activity-name')
                    author_elem = soup.select_one('#js_name') or soup.select_one('#js_profile_qrcode > div > strong')
                    content_elem = soup.select_one('#js_content')
                    
                    if content_elem:
                        # ç§»é™¤æ— ç”¨å…ƒç´ 
                        for remove_elem in content_elem.select('script, style, svg'):
                            remove_elem.extract()
                            
                        # å°è¯•è·å–æ‰€æœ‰æ–‡æœ¬
                        text_content = content_elem.get_text(separator='\n', strip=True)
                        
                        if text_content and len(text_content) > 200:  # å†…å®¹è¶³å¤Ÿé•¿
                            title = title_elem.get_text(strip=True) if title_elem else ""
                            author = author_elem.get_text(strip=True) if author_elem else "æœªçŸ¥ä½œè€…"
                            
                            # æ„å»ºå®Œæ•´å†…å®¹
                            full_content = ""
                            if title:
                                full_content += f"æ ‡é¢˜: {title}\n"
                            if author and author != "æœªçŸ¥ä½œè€…":
                                full_content += f"ä½œè€…: {author}\n"
                            full_content += f"\n{text_content}"
                            
                            logger.debug(f"[JinaSum] æˆåŠŸé€šè¿‡ç›´æ¥è¯·æ±‚æå–å¾®ä¿¡æ–‡ç« å†…å®¹ï¼Œé•¿åº¦: {len(text_content)}")
                            return full_content
                except Exception as e:
                    logger.error(f"[JinaSum] ç›´æ¥è¯·æ±‚æå–å¾®ä¿¡æ–‡ç« å¤±è´¥: {str(e)}")
                    # å¤±è´¥åä½¿ç”¨newspaperå°è¯•ï¼Œä¸è¦è¿”å›
            
            # é…ç½®newspaper
            newspaper.Config().browser_user_agent = selected_ua
            newspaper.Config().request_timeout = 30
            newspaper.Config().fetch_images = False  # ä¸ä¸‹è½½å›¾ç‰‡ä»¥åŠ å¿«é€Ÿåº¦
            newspaper.Config().memoize_articles = False  # é¿å…ç¼“å­˜å¯¼è‡´çš„é—®é¢˜
            
            # å¯¹newspaperçš„ä¸‹è½½è¿‡ç¨‹è¿›è¡Œå®šåˆ¶
            try:
                # åˆ›å»ºArticleå¯¹è±¡ä½†ä¸ç«‹å³ä¸‹è½½
                article = Article(url, language='zh')
                
                # æ‰‹åŠ¨ä¸‹è½½
                session = requests.Session()
                response = session.get(url, headers=headers, timeout=30)
                response.raise_for_status()
                
                # æ‰‹åŠ¨è®¾ç½®htmlå†…å®¹
                article.html = response.text
                article.download_state = 2  # è¡¨ç¤ºä¸‹è½½å®Œæˆ
                
                # ç„¶åè§£æ
                article.parse()
            except Exception as direct_dl_error:
                logger.error(f"[JinaSum] å°è¯•å®šåˆ¶ä¸‹è½½å¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†æ–¹æ³•: {str(direct_dl_error)}")
                article = Article(url, language='zh')
                article.download()
                article.parse()
            
            # å°è¯•è·å–å®Œæ•´å†…å®¹
            title = article.title
            authors = ', '.join(article.authors) if article.authors else "æœªçŸ¥ä½œè€…"
            publish_date = article.publish_date.strftime("%Y-%m-%d") if article.publish_date else "æœªçŸ¥æ—¥æœŸ"
            content = article.text
            
            # å¦‚æœå†…å®¹ä¸ºç©ºæˆ–è¿‡çŸ­ï¼Œå°è¯•ç›´æ¥ä»HTMLè·å–
            if not content or len(content) < 500:
                logger.debug("[JinaSum] Article content too short, trying to extract from HTML directly")
                try:
                    from bs4 import BeautifulSoup
                    soup = BeautifulSoup(article.html, 'html.parser')
                    
                    # ç§»é™¤è„šæœ¬å’Œæ ·å¼å…ƒç´ 
                    for script in soup(["script", "style"]):
                        script.extract()
                    
                    # è·å–æ‰€æœ‰æ–‡æœ¬
                    text = soup.get_text(separator='\n', strip=True)
                    
                    # å¦‚æœç›´æ¥æå–çš„å†…å®¹æ›´é•¿ï¼Œä½¿ç”¨å®ƒ
                    if len(text) > len(content):
                        content = text
                        logger.debug(f"[JinaSum] Using BeautifulSoup extracted content: {len(content)} chars")
                except Exception as bs_error:
                    logger.error(f"[JinaSum] BeautifulSoup extraction failed: {str(bs_error)}")
            
            # åˆæˆæœ€ç»ˆå†…å®¹
            if title:
                full_content = f"æ ‡é¢˜: {title}\n"
                if authors and authors != "æœªçŸ¥ä½œè€…":
                    full_content += f"ä½œè€…: {authors}\n"
                if publish_date and publish_date != "æœªçŸ¥æ—¥æœŸ":
                    full_content += f"å‘å¸ƒæ—¥æœŸ: {publish_date}\n"
                full_content += f"\n{content}"
            else:
                full_content = content
            
            if not full_content or len(full_content.strip()) < 50:
                logger.debug("[JinaSum] No content extracted by newspaper")
                return None
            
            # å¯¹äºBç«™è§†é¢‘ï¼Œå°è¯•è·å–è§†é¢‘æè¿°
            if "bilibili.com" in url or "b23.tv" in url:
                if title and not content:
                    # å¦‚æœåªæœ‰æ ‡é¢˜æ²¡æœ‰å†…å®¹ï¼Œè‡³å°‘è¿”å›æ ‡é¢˜
                    return f"æ ‡é¢˜: {title}\n\næè¿°: è¿™æ˜¯ä¸€ä¸ªBç«™è§†é¢‘ï¼Œæ— æ³•è·å–å®Œæ•´å†…å®¹ã€‚è¯·ç›´æ¥è§‚çœ‹è§†é¢‘ã€‚"
            
            logger.debug(f"[JinaSum] Successfully extracted content via newspaper, length: {len(full_content)}")
            return full_content
            
        except Exception as e:
            logger.error(f"[JinaSum] Error extracting content via newspaper: {str(e)}")
            if "mp.weixin.qq.com" in url:
                return f"æ— æ³•è·å–å¾®ä¿¡å…¬ä¼—å·æ–‡ç« å†…å®¹ã€‚å¯èƒ½åŸå› ï¼š\n1. æ–‡ç« éœ€è¦ç™»å½•æ‰èƒ½æŸ¥çœ‹\n2. æ–‡ç« å·²è¢«åˆ é™¤\n3. æœåŠ¡å™¨è¢«å¾®ä¿¡é£æ§\n\nè¯·å°è¯•ç›´æ¥æ‰“å¼€é“¾æ¥: {url}"
            return None

    def _process_summary(self, content: str, e_context: EventContext, retry_count: int = 0, skip_notice: bool = False):
        """å¤„ç†æ€»ç»“è¯·æ±‚"""
        try:
            if not self._check_url(content):
                logger.debug(f"[JinaSum] {content} is not a valid url, skip")
                return
                
            if retry_count == 0 and not skip_notice:
                logger.debug("[JinaSum] Processing URL: %s" % content)
                reply = Reply(ReplyType.TEXT, "ğŸ‰æ­£åœ¨ä¸ºæ‚¨ç”Ÿæˆæ€»ç»“ï¼Œè¯·ç¨å€™...")
                channel = e_context["channel"]
                channel.send(reply, e_context["context"])

            # è·å–ç½‘é¡µå†…å®¹
            target_url = html.unescape(content)
            target_url_content = None
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«XMLæ•°æ®ï¼ˆåˆ†äº«æ¶ˆæ¯é”™è¯¯ï¼‰
            if target_url.startswith("<") and "appmsg" in target_url:
                logger.warning("[JinaSum] æ£€æµ‹åˆ°XMLæ•°æ®è€Œä¸æ˜¯URLï¼Œå°è¯•æå–çœŸå®URL")
                try:
                    import xml.etree.ElementTree as ET
                    # å¤„ç†å¯èƒ½çš„XMLå£°æ˜
                    if target_url.startswith('<?xml'):
                        target_url = target_url[target_url.find('<msg>'):]
                    
                    root = ET.fromstring(target_url)
                    url_elem = root.find(".//url")
                    if url_elem is not None and url_elem.text:
                        target_url = url_elem.text
                        logger.debug(f"[JinaSum] ä»XMLä¸­æå–åˆ°URL: {target_url}")
                    else:
                        logger.error("[JinaSum] æ— æ³•ä»XMLä¸­æå–URL")
                        raise ValueError("æ— æ³•ä»åˆ†äº«å¡ç‰‡ä¸­æå–URL")
                except Exception as ex:
                    logger.error(f"[JinaSum] è§£æXMLå¤±è´¥: {str(ex)}")
                    raise ValueError("æ— æ³•ä»åˆ†äº«å¡ç‰‡ä¸­æå–URL")
            
            # ä½¿ç”¨newspaper3kæå–å†…å®¹
            logger.debug(f"[JinaSum] ä½¿ç”¨newspaper3kæå–å†…å®¹: {target_url}")
            target_url_content = self._get_content_via_newspaper(target_url)
            
            # æ£€æŸ¥è¿”å›çš„å†…å®¹æ˜¯å¦åŒ…å«éªŒè¯æç¤º
            if target_url_content and target_url_content.startswith("âš ï¸"):
                # è¿™æ˜¯ä¸€ä¸ªéªŒè¯æç¤ºï¼Œç›´æ¥è¿”å›ç»™ç”¨æˆ·
                logger.info(f"[JinaSum] è¿”å›éªŒè¯æç¤ºç»™ç”¨æˆ·: {target_url_content}")
                reply = Reply(ReplyType.INFO, target_url_content)
                e_context["reply"] = reply
                e_context.action = EventAction.BREAK_PASS
                return
            
            # å¦‚æœnewspaperæå–å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨API
            if not target_url_content:
                logger.debug(f"[JinaSum] newspaperæå–å¤±è´¥ï¼Œå°è¯•APIæ–¹æ³•: {target_url}")
                target_url_content = self._get_content_via_api(target_url)
            
            # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥
            if not target_url_content:
                # å¯¹äºBç«™è§†é¢‘ï¼Œæä¾›ç‰¹æ®Šå¤„ç†
                if "bilibili.com" in target_url or "b23.tv" in target_url:
                    target_url_content = "è¿™æ˜¯ä¸€ä¸ªBç«™è§†é¢‘é“¾æ¥ã€‚ç”±äºè§†é¢‘å†…å®¹æ— æ³•ç›´æ¥æå–ï¼Œè¯·ç›´æ¥ç‚¹å‡»é“¾æ¥è§‚çœ‹è§†é¢‘ã€‚"
                else:
                    raise ValueError("æ— æ³•æå–æ–‡ç« å†…å®¹")
                
            # æ¸…æ´—å†…å®¹
            target_url_content = self._clean_content(target_url_content)
            
            # é™åˆ¶å†…å®¹é•¿åº¦
            target_url_content = target_url_content[:self.max_words]
            logger.debug(f"[JinaSum] Got content length: {len(target_url_content)}")
            
            # æ„é€ æç¤ºè¯å’Œå†…å®¹
            sum_prompt = f"{self.prompt}\n\n'''{target_url_content}'''"
            
            try:
                # ç›´æ¥ä½¿ç”¨Bridgeè°ƒç”¨åå°è·å–å›å¤
                from bridge.bridge import Bridge
                
                # ä¿®æ”¹contextå†…å®¹
                e_context['context'].type = ContextType.TEXT
                e_context['context'].content = sum_prompt
                
                # ä½¿ç”¨Bridgeè°ƒç”¨åå°æ¨¡å‹
                logger.debug(f"[JinaSum] ä½¿ç”¨Bridgeç›´æ¥è°ƒç”¨åå°æ¨¡å‹ï¼Œprompté•¿åº¦={len(sum_prompt)}")
                bridge = Bridge()
                reply_content = bridge.fetch_reply_content(sum_prompt, e_context['context'])
                
                # æ£€æŸ¥è¿”å›å†…å®¹
                if reply_content and hasattr(reply_content, 'content'):
                    reply = reply_content  # å¦‚æœè¿”å›çš„æ˜¯Replyå¯¹è±¡ï¼Œç›´æ¥ä½¿ç”¨
                else:
                    # å¦åˆ™åˆ›å»ºæ–°çš„Replyå¯¹è±¡
                    if isinstance(reply_content, str):
                        reply = Reply(ReplyType.TEXT, reply_content)
                    else:
                        reply = Reply(ReplyType.ERROR, "åå°è¿”å›æ ¼å¼é”™è¯¯")
                
                # è®¾ç½®å›å¤å¹¶ä¸­æ–­å¤„ç†é“¾
                e_context["reply"] = reply
                e_context.action = EventAction.BREAK_PASS
                logger.debug(f"[JinaSum] ä½¿ç”¨Bridgeç›´æ¥è°ƒç”¨åå°æ¨¡å‹æˆåŠŸï¼Œå›å¤ç±»å‹={reply.type}ï¼Œé•¿åº¦={len(reply.content) if reply.content else 0}")
                return
                
            except Exception as e:
                logger.warning(f"[JinaSum] ç›´æ¥è°ƒç”¨åå°å¤±è´¥: {str(e)}", exc_info=True)
                
                # å¦‚æœç›´æ¥è°ƒç”¨å¤±è´¥ï¼Œå›é€€åˆ°æ’ä»¶é“¾çš„æ–¹å¼
                logger.debug("[JinaSum] å›é€€åˆ°ä½¿ç”¨æ’ä»¶é“¾å¤„ç†")
                e_context.action = EventAction.CONTINUE
                return
                
        except Exception as e:
            logger.error(f"[JinaSum] Error in processing summary: {str(e)}")
            if retry_count < 3:
                logger.info(f"[JinaSum] Retrying {retry_count + 1}/3...")
                return self._process_summary(content, e_context, retry_count + 1, True)
            
            # å‹å¥½çš„é”™è¯¯æç¤º
            error_msg = "æŠ±æ­‰ï¼Œæ— æ³•è·å–æ–‡ç« å†…å®¹ã€‚å¯èƒ½æ˜¯å› ä¸º:\n"
            error_msg += "1. æ–‡ç« éœ€è¦ç™»å½•æˆ–å·²è¿‡æœŸ\n"
            error_msg += "2. æ–‡ç« æœ‰ç‰¹æ®Šçš„è®¿é—®é™åˆ¶\n"
            error_msg += "3. ç½‘ç»œè¿æ¥ä¸ç¨³å®š\n\n"
            error_msg += "å»ºè®®æ‚¨:\n"
            error_msg += "- ç›´æ¥æ‰“å¼€é“¾æ¥æŸ¥çœ‹\n"
            error_msg += "- ç¨åé‡è¯•\n"
            error_msg += "- å°è¯•å…¶ä»–æ–‡ç« "
            
            reply = Reply(ReplyType.ERROR, error_msg)
            e_context["reply"] = reply
            e_context.action = EventAction.BREAK_PASS

    def _process_question(self, question: str, chat_id: str, e_context: EventContext, retry_count: int = 0):
        """å¤„ç†ç”¨æˆ·æé—®"""
        try:
            # è·å–æœ€è¿‘æ€»ç»“çš„å†…å®¹
            recent_content = None
            recent_timestamp = 0
            
            # éå†æ‰€æœ‰ç¼“å­˜æ‰¾åˆ°æœ€è¿‘æ€»ç»“çš„å†…å®¹
            for url, cache_data in self.content_cache.items():
                if cache_data["timestamp"] > recent_timestamp:
                    recent_timestamp = cache_data["timestamp"]
                    recent_content = cache_data["content"]
            
            if not recent_content or time.time() - recent_timestamp > self.content_cache_timeout:
                logger.debug(f"[JinaSum] No valid content cache found or content expired")
                return  # æ‰¾ä¸åˆ°ç›¸å…³æ–‡ç« ï¼Œè®©åç»­æ’ä»¶å¤„ç†é—®é¢˜
            
            if retry_count == 0:
                reply = Reply(ReplyType.TEXT, "ğŸ¤” æ­£åœ¨æ€è€ƒæ‚¨çš„é—®é¢˜ï¼Œè¯·ç¨å€™...")
                channel = e_context["channel"]
                channel.send(reply, e_context["context"])

            # å‡†å¤‡é—®ç­”è¯·æ±‚
            openai_chat_url = self._get_openai_chat_url()
            openai_headers = self._get_openai_headers()
            
            # æ„å»ºé—®ç­”çš„ prompt
            qa_prompt = self.qa_prompt.format(
                content=recent_content[:self.max_words],
                question=question
            )
            
            openai_payload = {
                'model': self.open_ai_model,
                'messages': [{"role": "user", "content": qa_prompt}]
            }
            
            # è°ƒç”¨ API è·å–å›ç­”
            response = requests.post(openai_chat_url, headers=openai_headers, json=openai_payload, timeout=60)
            response.raise_for_status()
            answer = response.json()['choices'][0]['message']['content']
            
            reply = Reply(ReplyType.TEXT, answer)
            e_context["reply"] = reply
            e_context.action = EventAction.BREAK_PASS
            
        except Exception as e:
            logger.error(f"[JinaSum] Error in processing question: {str(e)}")
            if retry_count < 3:
                return self._process_question(question, chat_id, e_context, retry_count + 1)
            reply = Reply(ReplyType.ERROR, f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‡ºé”™: {str(e)}")
            e_context["reply"] = reply
            e_context.action = EventAction.BREAK_PASS

    def get_help_text(self, verbose, **kwargs):
        help_text = "ç½‘é¡µå†…å®¹æ€»ç»“æ’ä»¶:\n"
        help_text += "1. å‘é€ã€Œæ€»ç»“ ç½‘å€ã€å¯ä»¥æ€»ç»“æŒ‡å®šç½‘é¡µçš„å†…å®¹\n"
        help_text += "2. å•èŠæ—¶åˆ†äº«æ¶ˆæ¯ä¼šè‡ªåŠ¨æ€»ç»“\n"
        if self.auto_sum:
            help_text += "3. ç¾¤èŠä¸­åˆ†äº«æ¶ˆæ¯é»˜è®¤è‡ªåŠ¨æ€»ç»“"
            if self.black_group_list:
                help_text += "ï¼ˆéƒ¨åˆ†ç¾¤ç»„éœ€è¦å‘é€å«ã€Œæ€»ç»“ã€çš„æ¶ˆæ¯è§¦å‘ï¼‰\n"
            else:
                help_text += "\n"
        else:
            help_text += "3. ç¾¤èŠä¸­æ”¶åˆ°åˆ†äº«æ¶ˆæ¯åï¼Œå‘é€åŒ…å«ã€Œæ€»ç»“ã€çš„æ¶ˆæ¯å³å¯è§¦å‘æ€»ç»“\n"
        help_text += f"4. æ€»ç»“å®Œæˆå5åˆ†é’Ÿå†…ï¼Œå¯ä»¥å‘é€ã€Œ{self.qa_trigger}xxxã€æ¥è¯¢é—®æ–‡ç« ç›¸å…³é—®é¢˜\n"
        help_text += "æ³¨ï¼šç¾¤èŠä¸­çš„åˆ†äº«æ¶ˆæ¯çš„æ€»ç»“è¯·æ±‚éœ€è¦åœ¨60ç§’å†…å‘å‡º"
        return help_text

    def _load_config_template(self):
        """åŠ è½½é…ç½®æ¨¡æ¿"""
        try:
            template_path = os.path.join(os.path.dirname(__file__), "config.json.template")
            if os.path.exists(template_path):
                with open(template_path, "r", encoding="utf-8") as f:
                    plugin_conf = json.load(f)
                    return plugin_conf
        except Exception as e:
            logger.exception(e)

    def _get_openai_chat_url(self):
        return self.open_ai_api_base + "/chat/completions"

    def _get_openai_headers(self):
        """è·å–openaiçš„header"""
        config = super().get_config()
        return {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {config.get('openai_api_key')}"
        }

    def _get_openai_payload(self, target_url_content):
        """æ„é€ openaiçš„payload
        
        Args:
            target_url_content: ç½‘é¡µå†…å®¹
        """
        sum_prompt = f"{self.prompt}\n\n'''{target_url_content}'''"
        messages = [{"role": "user", "content": sum_prompt}]
        payload = {
            'model': self.open_ai_model,
            'messages': messages
        }
        return payload

    def _check_url(self, target_url: str):
        """æ£€æŸ¥URLæ˜¯å¦æœ‰æ•ˆä¸”å…è®¸è®¿é—®
        
        Args:
            target_url: è¦æ£€æŸ¥çš„URL
            
        Returns:
            bool: URLæ˜¯å¦æœ‰æ•ˆä¸”å…è®¸è®¿é—®
        """
        stripped_url = target_url.strip()
        logger.debug(f"[JinaSum] æ£€æŸ¥URL: {stripped_url}")
        
        # ç®€å•æ ¡éªŒæ˜¯å¦æ˜¯url
        if not stripped_url.startswith("http://") and not stripped_url.startswith("https://"):
            logger.debug("[JinaSum] URLä¸ä»¥http://æˆ–https://å¼€å¤´ï¼Œè·³è¿‡")
            return False

        # æ£€æµ‹ä¸€äº›å¸¸è§çš„ä¸é€‚åˆæ€»ç»“çš„å†…å®¹ç±»å‹
        skip_patterns = [
            # è§†é¢‘/éŸ³ä¹å¹³å°çš„éæ–‡ç« å†…å®¹
            r"(bilibili\.com|b23\.tv).*/video/", # Bç«™è§†é¢‘
            r"(youtube\.com|youtu\.be)/watch", # YouTubeè§†é¢‘
            r"(music\.163\.com|y\.qq\.com)/(song|playlist|album)", # éŸ³ä¹
            
            # æ–‡ä»¶é“¾æ¥
            r"\.(pdf|doc|docx|ppt|pptx|xls|xlsx|zip|rar|7z)(\?|$)", # æ–‡æ¡£å’Œå‹ç¼©åŒ…
            
            # å›¾ç‰‡é“¾æ¥
            r"\.(jpg|jpeg|png|gif|bmp|webp|svg)(\?|$)", # å›¾ç‰‡
            
            # åœ°å›¾
            r"(map\.(baidu|google|qq)\.com)", # åœ°å›¾
            
            # å·¥å…·ç±»
            r"(docs\.qq\.com|shimo\.im|yuque\.com|notion\.so)", # åœ¨çº¿æ–‡æ¡£
            
            # ç¤¾äº¤åª’ä½“ç‰¹å®šå†…å®¹
            r"weixin\.qq\.com/[^/]+/([^/]+/){2,}",  # å¾®ä¿¡å°ç¨‹åºæˆ–å…¶ä»–åŠŸèƒ½
            r"(weibo\.com|t\.cn)/[^/]+/[^/]+",  # å¾®åš
            
            # å•†åŸå•†å“
            r"(taobao\.com|tmall\.com|jd\.com)/.*?(item|product)",  # ç”µå•†å•†å“
            
            # å°ç¨‹åº
            r"servicewechat\.com"  # å¾®ä¿¡å°ç¨‹åº
        ]
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ£€æŸ¥
        import re
        for pattern in skip_patterns:
            if re.search(pattern, stripped_url, re.IGNORECASE):
                logger.debug(f"[JinaSum] URLåŒ¹é…è·³è¿‡æ¨¡å¼: {pattern}")
                return False

        # æ£€æŸ¥ç™½åå•
        if len(self.white_url_list):
            if not any(stripped_url.startswith(white_url) for white_url in self.white_url_list):
                logger.debug("[JinaSum] URLä¸åœ¨ç™½åå•ä¸­")
                return False

        # æ’é™¤é»‘åå•ï¼Œé»‘åå•ä¼˜å…ˆçº§>ç™½åå•
        for black_url in self.black_url_list:
            if stripped_url.startswith(black_url):
                logger.debug(f"[JinaSum] URLåœ¨é»‘åå•ä¸­: {black_url}")
                return False

        logger.debug("[JinaSum] URLæ£€æŸ¥é€šè¿‡")
        return True

    def _clean_content(self, content: str) -> str:
        """æ¸…æ´—å†…å®¹ï¼Œå»é™¤å›¾ç‰‡ã€é“¾æ¥ã€å¹¿å‘Šç­‰æ— ç”¨ä¿¡æ¯
        
        Args:
            content: åŸå§‹å†…å®¹
            
        Returns:
            str: æ¸…æ´—åçš„å†…å®¹
        """
        # è®°å½•åŸå§‹é•¿åº¦
        original_length = len(content)
        logger.debug(f"[JinaSum] Original content length: {original_length}")
        
        # ç§»é™¤Markdownå›¾ç‰‡æ ‡ç­¾
        content = re.sub(r'!\[.*?\]\(.*?\)', '', content)
        content = re.sub(r'\[!\[.*?\]\(.*?\)', '', content)  # åµŒå¥—å›¾ç‰‡æ ‡ç­¾
        
        # ç§»é™¤å›¾ç‰‡æè¿° (é€šå¸¸åœ¨æ–¹æ‹¬å·æˆ–ç‰¹å®šæ ¼å¼ä¸­)
        content = re.sub(r'\[å›¾ç‰‡\]|\[image\]|\[img\]|\[picture\]', '', content, flags=re.IGNORECASE)
        content = re.sub(r'\[.*?å›¾ç‰‡.*?\]', '', content)
        
        # ç§»é™¤é˜…è¯»æ—¶é—´ã€å­—æ•°ç­‰å…ƒæ•°æ®
        content = re.sub(r'æœ¬æ–‡å­—æ•°ï¼š\d+ï¼Œé˜…è¯»æ—¶é•¿å¤§çº¦\d+åˆ†é’Ÿ', '', content)
        content = re.sub(r'é˜…è¯»æ—¶é•¿[:ï¼š].*?åˆ†é’Ÿ', '', content)
        content = re.sub(r'å­—æ•°[:ï¼š]\d+', '', content)
        
        # ç§»é™¤æ—¥æœŸæ ‡è®°å’Œæ—¶é—´æˆ³
        content = re.sub(r'\d{4}[\.å¹´/-]\d{1,2}[\.æœˆ/-]\d{1,2}[æ—¥å·]?(\s+\d{1,2}:\d{1,2}(:\d{1,2})?)?', '', content)
        
        # ç§»é™¤åˆ†éš”çº¿
        content = re.sub(r'\*\s*\*\s*\*', '', content)
        content = re.sub(r'-{3,}', '', content)
        content = re.sub(r'_{3,}', '', content)
        
        # ç§»é™¤ç½‘é¡µä¸­å¸¸è§çš„å¹¿å‘Šæ ‡è®°
        ad_patterns = [
            r'å¹¿å‘Š\s*[\.ã€‚]?', 
            r'èµåŠ©å†…å®¹', 
            r'sponsored content',
            r'advertisement',
            r'promoted content',
            r'æ¨å¹¿ä¿¡æ¯',
            r'\[å¹¿å‘Š\]',
            r'ã€å¹¿å‘Šã€‘',
        ]
        for pattern in ad_patterns:
            content = re.sub(pattern, '', content, flags=re.IGNORECASE)
        
        # ç§»é™¤URLé“¾æ¥å’Œç©ºçš„Markdowné“¾æ¥
        content = re.sub(r'https?://\S+', '', content)
        content = re.sub(r'www\.\S+', '', content)
        content = re.sub(r'\[\]\(.*?\)', '', content)  # ç©ºé“¾æ¥å¼•ç”¨ [](...)
        content = re.sub(r'\[.+?\]\(\s*\)', '', content)  # æœ‰æ–‡æœ¬æ— é“¾æ¥ [text]()
        
        # æ¸…ç†Markdownæ ¼å¼ä½†ä¿ç•™æ–‡æœ¬å†…å®¹
        content = re.sub(r'\*\*(.+?)\*\*', r'\1', content)  # ç§»é™¤åŠ ç²—æ ‡è®°ä½†ä¿ç•™å†…å®¹
        content = re.sub(r'\*(.+?)\*', r'\1', content)      # ç§»é™¤æ–œä½“æ ‡è®°ä½†ä¿ç•™å†…å®¹
        content = re.sub(r'`(.+?)`', r'\1', content)        # ç§»é™¤ä»£ç æ ‡è®°ä½†ä¿ç•™å†…å®¹
        
        # æ¸…ç†æ–‡ç« å°¾éƒ¨çš„"å¾®ä¿¡ç¼–è¾‘"å’Œ"æ¨èé˜…è¯»"ç­‰æ— å…³å†…å®¹
        content = re.sub(r'\*\*å¾®ä¿¡ç¼–è¾‘\*\*.*?$', '', content, flags=re.MULTILINE)
        content = re.sub(r'\*\*æ¨èé˜…è¯»\*\*.*?$', '', content, flags=re.MULTILINE | re.DOTALL)
        
        # æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
        content = re.sub(r'\n{3,}', '\n\n', content)  # ç§»é™¤å¤šä½™ç©ºè¡Œ
        content = re.sub(r'\s{2,}', ' ', content)     # ç§»é™¤å¤šä½™ç©ºæ ¼
        content = re.sub(r'^\s+', '', content, flags=re.MULTILINE)  # ç§»é™¤è¡Œé¦–ç©ºç™½
        content = re.sub(r'\s+$', '', content, flags=re.MULTILINE)  # ç§»é™¤è¡Œå°¾ç©ºç™½
        
        # è®°å½•æ¸…æ´—åé•¿åº¦
        cleaned_length = len(content)
        logger.debug(f"[JinaSum] Cleaned content length: {cleaned_length}, removed {original_length - cleaned_length} characters")
        
        return content