# encoding:utf-8
import json
import os
import html
import re
from urllib.parse import urlparse, quote, parse_qs, quote_plus
import time
import asyncio
import nest_asyncio

import requests
from newspaper import Article
import newspaper
from bs4 import BeautifulSoup
from requests_html import HTMLSession

import plugins
from bridge.context import ContextType
from bridge.reply import Reply, ReplyType
from common.log import logger
from plugins import *

# åº”ç”¨nest_asyncioä»¥è§£å†³äº‹ä»¶å¾ªç¯é—®é¢˜
try:
    nest_asyncio.apply()
except Exception as e:
    logger.warning(f"[JinaSum] æ— æ³•åº”ç”¨nest_asyncio: {str(e)}")

@plugins.register(
    name="JinaSum",
    desire_priority=20,
    hidden=False,
    desc="Sum url link content with newspaper3k and llm",
    version="2.3",
    author="sofs2005",
)
class JinaSum(Plugin):
    """ç½‘é¡µå†…å®¹æ€»ç»“æ’ä»¶
    
    åŠŸèƒ½ï¼š
    1. è‡ªåŠ¨æ€»ç»“åˆ†äº«çš„ç½‘é¡µå†…å®¹
    2. æ”¯æŒæ‰‹åŠ¨è§¦å‘æ€»ç»“
    3. æ”¯æŒç¾¤èŠå’Œå•èŠä¸åŒå¤„ç†æ–¹å¼
    4. æ”¯æŒé»‘åå•ç¾¤ç»„é…ç½®
    """
    # é»˜è®¤é…ç½®
    DEFAULT_CONFIG = {
        "max_words": 8000,
        "prompt": "æˆ‘éœ€è¦å¯¹ä¸‹é¢å¼•å·å†…æ–‡æ¡£è¿›è¡Œæ€»ç»“ï¼Œæ€»ç»“è¾“å‡ºåŒ…æ‹¬ä»¥ä¸‹ä¸‰ä¸ªéƒ¨åˆ†ï¼š\nğŸ“– ä¸€å¥è¯æ€»ç»“\nğŸ”‘ å…³é”®è¦ç‚¹,ç”¨æ•°å­—åºå·åˆ—å‡º3-5ä¸ªæ–‡ç« çš„æ ¸å¿ƒå†…å®¹\nğŸ· æ ‡ç­¾: #xx #xx\nè¯·ä½¿ç”¨emojiè®©ä½ çš„è¡¨è¾¾æ›´ç”ŸåŠ¨\n\n",
        "white_url_list": [],
        "black_url_list": [
            "https://support.weixin.qq.com",  # è§†é¢‘å·è§†é¢‘
            "https://channels-aladin.wxqcloud.qq.com",  # è§†é¢‘å·éŸ³ä¹
        ],
        "black_group_list": [],
        "auto_sum": True,
        "cache_timeout": 300,  # ç¼“å­˜è¶…æ—¶æ—¶é—´ï¼ˆ5åˆ†é’Ÿï¼‰
    }

    def __init__(self):
        """åˆå§‹åŒ–æ’ä»¶é…ç½®"""
        try:
            super().__init__()
            
            # ç¡®ä¿ä½¿ç”¨é»˜è®¤é…ç½®åˆå§‹åŒ–
            self.config = super().load_config()
            if not self.config:
                self.config = self._load_config_template()
            
            # ä½¿ç”¨é»˜è®¤é…ç½®åˆå§‹åŒ–
            for key, default_value in self.DEFAULT_CONFIG.items():
                if key not in self.config:
                    self.config[key] = default_value
            
            # è®¾ç½®é…ç½®å‚æ•°
            self.max_words = self.config.get("max_words", 8000)
            self.prompt = self.config.get("prompt", "æˆ‘éœ€è¦å¯¹ä¸‹é¢å¼•å·å†…æ–‡æ¡£è¿›è¡Œæ€»ç»“...")
            self.cache_timeout = self.config.get("cache_timeout", 300)  # é»˜è®¤5åˆ†é’Ÿ
            
            # URLé»‘ç™½åå•é…ç½®
            self.white_url_list = self.config.get("white_url_list", [])
            self.black_url_list = self.config.get("black_url_list", [])
            self.black_group_list = self.config.get("black_group_list", [])
            
            # æ˜¯å¦è‡ªåŠ¨æ€»ç»“ï¼ˆä»…ç¾¤èŠæœ‰æ•ˆï¼‰
            self.auto_sum = self.config.get("auto_sum", False)
            
            # æ¶ˆæ¯ç¼“å­˜
            self.pending_messages = {}  # ç”¨äºå­˜å‚¨å¾…å¤„ç†çš„æ¶ˆæ¯ï¼Œæ ¼å¼: {chat_id: {"content": content, "timestamp": time.time()}}
            
            # API è®¾ç½®
            self.open_ai_api_base = "https://api.openai.com/v1"
            self.open_ai_model = "gpt-3.5-turbo"
            
            logger.info(f"[JinaSum] åˆå§‹åŒ–å®Œæˆ, config={self.config}")
            self.handlers[Event.ON_HANDLE_CONTEXT] = self.on_handle_context
        except Exception as e:
            logger.error(f"[JinaSum] åˆå§‹åŒ–å¼‚å¸¸ï¼š{str(e)}", exc_info=True)
            raise Exception("[JinaSum] åˆå§‹åŒ–å¤±è´¥")

    def on_handle_context(self, e_context: EventContext):
        """å¤„ç†æ¶ˆæ¯"""
        context = e_context['context']
        logger.info(f"[JinaSum] æ”¶åˆ°æ¶ˆæ¯, ç±»å‹={context.type}, å†…å®¹é•¿åº¦={len(context.content)}")

        # é¦–å…ˆåœ¨æ—¥å¿—ä¸­è®°å½•å®Œæ•´çš„æ¶ˆæ¯å†…å®¹ï¼Œä¾¿äºè°ƒè¯•
        orig_content = context.content
        if len(orig_content) > 500:
            logger.info(f"[JinaSum] æ¶ˆæ¯å†…å®¹(æˆªæ–­): {orig_content[:500]}...")
        else:
            logger.info(f"[JinaSum] æ¶ˆæ¯å†…å®¹: {orig_content}")
        
        if context.type not in [ContextType.TEXT, ContextType.SHARING]:
            logger.info(f"[JinaSum] æ¶ˆæ¯ç±»å‹ä¸ç¬¦åˆå¤„ç†æ¡ä»¶ï¼Œè·³è¿‡: {context.type}")
            return

        content = context.content
        channel = e_context['channel']
        msg = e_context['context']['msg']
        chat_id = msg.from_user_id
        is_group = msg.is_group
        
        # æ‰“å°å‰50ä¸ªå­—ç¬¦ç”¨äºè°ƒè¯•
        preview = content[:50] + "..." if len(content) > 50 else content
        logger.info(f"[JinaSum] å¤„ç†æ¶ˆæ¯: {preview}, ç±»å‹={context.type}")

        # æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸ºXMLæ ¼å¼ï¼ˆå“”å“©å“”å“©ç­‰ç¬¬ä¸‰æ–¹åˆ†äº«å¡ç‰‡ï¼‰
        if content.startswith('<?xml') or (content.startswith('<msg>') and '<appmsg' in content) or ('<appmsg' in content and '<url>' in content):
            logger.info("[JinaSum] æ£€æµ‹åˆ°XMLæ ¼å¼åˆ†äº«å¡ç‰‡ï¼Œå°è¯•æå–URL")
            try:
                import xml.etree.ElementTree as ET
                # å¤„ç†å¯èƒ½çš„XMLå£°æ˜
                if content.startswith('<?xml'):
                    content = content[content.find('<msg>'):]
                
                # å¦‚æœä¸æ˜¯å®Œæ•´çš„XMLï¼Œå°è¯•æ·»åŠ æ ¹èŠ‚ç‚¹
                if not content.startswith('<msg') and '<appmsg' in content:
                    content = f"<msg>{content}</msg>"
                
                # å¯¹äºä¸€äº›å¯èƒ½æ ¼å¼ä¸æ ‡å‡†çš„XMLï¼Œä½¿ç”¨æ›´å®½æ¾çš„è§£ææ–¹å¼
                try:
                    root = ET.fromstring(content)
                except ET.ParseError:
                    # å°è¯•ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–URL
                    import re
                    url_match = re.search(r'<url>(.*?)</url>', content)
                    if url_match:
                        extracted_url = url_match.group(1)
                        logger.info(f"[JinaSum] é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼ä»XMLä¸­æå–åˆ°URL: {extracted_url}")
                        content = extracted_url
                        context.type = ContextType.SHARING
                        context.content = extracted_url
                    else:
                        logger.error("[JinaSum] æ— æ³•é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼ä»XMLä¸­æå–URL")
                        return
                else:
                    # XMLè§£ææˆåŠŸ
                    url_elem = root.find('.//url')
                    title_elem = root.find('.//title')
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰appinfoèŠ‚ç‚¹ï¼Œåˆ¤æ–­æ˜¯å¦ä¸ºBç«™ç­‰ç‰¹æ®Šåº”ç”¨
                    appinfo = root.find('.//appinfo')
                    app_name = None
                    if appinfo is not None and appinfo.find('appname') is not None:
                        app_name = appinfo.find('appname').text
                        logger.info(f"[JinaSum] æ£€æµ‹åˆ°APPåˆ†äº«: {app_name}")
                    
                    logger.info(f"[JinaSum] XMLè§£æç»“æœ: url_elem={url_elem is not None}, title_elem={title_elem is not None}, app_name={app_name}")
                    
                    if url_elem is not None and url_elem.text:
                        # æå–åˆ°URLï¼Œå°†ç±»å‹ä¿®æ”¹ä¸ºSHARING
                        extracted_url = url_elem.text
                        logger.info(f"[JinaSum] ä»XMLä¸­æå–åˆ°URL: {extracted_url}")
                        content = extracted_url
                        context.type = ContextType.SHARING
                        context.content = extracted_url
                        
                        # å¯¹äºBç«™è§†é¢‘é“¾æ¥ï¼Œè®°å½•é¢å¤–ä¿¡æ¯
                        if app_name and ("å“”å“©å“”å“©" in app_name or "bilibili" in app_name.lower() or "bç«™" in app_name):
                            logger.info("[JinaSum] æ£€æµ‹åˆ°Bç«™è§†é¢‘åˆ†äº«")
                            # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ Bç«™è§†é¢‘çš„ç‰¹æ®Šå¤„ç†é€»è¾‘
                    else:
                        logger.error("[JinaSum] æ— æ³•ä»XMLä¸­æå–URL")
                        return
            except Exception as e:
                logger.error(f"[JinaSum] è§£æXMLå¤±è´¥: {str(e)}", exc_info=True)
                return

        # æ£€æŸ¥æ˜¯å¦éœ€è¦è‡ªåŠ¨æ€»ç»“
        should_auto_sum = self.auto_sum
        if should_auto_sum and is_group and msg.from_user_nickname in self.black_group_list:
            should_auto_sum = False

        # æ¸…ç†è¿‡æœŸç¼“å­˜
        self._clean_expired_cache()

        # å¤„ç†åˆ†äº«æ¶ˆæ¯
        if context.type == ContextType.SHARING:
            logger.debug("[JinaSum] Processing SHARING message")
            if is_group:
                if should_auto_sum:
                    return self._process_summary(content, e_context, retry_count=0)
                else:
                    self.pending_messages[chat_id] = {
                        "content": content,
                        "timestamp": time.time()
                    }
                    logger.debug(f"[JinaSum] Cached SHARING message: {content}, chat_id={chat_id}")
                    return
            else:  # å•èŠæ¶ˆæ¯ç›´æ¥å¤„ç†
                return self._process_summary(content, e_context, retry_count=0)

        # å¤„ç†æ–‡æœ¬æ¶ˆæ¯
        elif context.type == ContextType.TEXT:
            logger.debug("[JinaSum] Processing TEXT message")
            content = content.strip()
            
            # ç§»é™¤å¯èƒ½çš„@ä¿¡æ¯
            if content.startswith("@"):
                parts = content.split(" ", 1)
                if len(parts) > 1:
                    content = parts[1].strip()
                else:
                    content = ""
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«"æ€»ç»“"å…³é”®è¯ï¼ˆä»…ç¾¤èŠéœ€è¦ï¼‰
            if is_group and "æ€»ç»“" in content:
                logger.debug(f"[JinaSum] Found summary trigger, pending_messages={self.pending_messages}")
                if chat_id in self.pending_messages:
                    cached_content = self.pending_messages[chat_id]["content"]
                    logger.debug(f"[JinaSum] Processing cached content: {cached_content}")
                    del self.pending_messages[chat_id]
                    return self._process_summary(cached_content, e_context, retry_count=0, skip_notice=False)
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯ç›´æ¥URLæ€»ç»“ï¼Œç§»é™¤"æ€»ç»“"å¹¶æ£€æŸ¥å‰©ä½™å†…å®¹æ˜¯å¦ä¸ºURL
                url = content.replace("æ€»ç»“", "").strip()
                if url and self._check_url(url):
                    logger.debug(f"[JinaSum] Processing direct URL: {url}")
                    return self._process_summary(url, e_context, retry_count=0, skip_notice=False)
                logger.debug("[JinaSum] No content to summarize")
                return

                    
            # å•èŠä¸­ç›´æ¥å¤„ç†URL
            if not is_group and self._check_url(content):
                return self._process_summary(content, e_context, retry_count=0)

    def _clean_expired_cache(self):
        """æ¸…ç†è¿‡æœŸçš„ç¼“å­˜"""
        current_time = time.time()
        # æ¸…ç†å¾…å¤„ç†æ¶ˆæ¯ç¼“å­˜
        expired_keys = [
            k for k, v in self.pending_messages.items() 
            if current_time - v["timestamp"] > self.cache_timeout
        ]
        for k in expired_keys:
            del self.pending_messages[k]

    def _get_content_via_newspaper(self, url):
        """ä½¿ç”¨newspaper3kåº“æå–æ–‡ç« å†…å®¹
        
        Args:
            url: æ–‡ç« URL
            
        Returns:
            str: æ–‡ç« å†…å®¹,å¤±è´¥è¿”å›None
        """
        try:
            # å¤„ç†Bç«™çŸ­é“¾æ¥
            if "b23.tv" in url:
                # å…ˆè·å–é‡å®šå‘åçš„çœŸå®URL
                try:
                    logger.debug(f"[JinaSum] Resolving Bç«™çŸ­é“¾æ¥: {url}")
                    headers = {
                        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
                        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
                        "Cache-Control": "max-age=0",
                        "Connection": "keep-alive"
                    }
                    response = requests.head(url, headers=headers, allow_redirects=True, timeout=10)
                    if response.status_code == 200:
                        real_url = response.url
                        logger.debug(f"[JinaSum] Bç«™çŸ­é“¾æ¥è§£æç»“æœ: {real_url}")
                        url = real_url
                except Exception as e:
                    logger.error(f"[JinaSum] è§£æBç«™çŸ­é“¾æ¥å¤±è´¥: {str(e)}")
            
            # å¢å¼ºæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨è®¿é—®
            import random
            
            # éšæœºé€‰æ‹©ä¸€ä¸ªUser-Agentï¼Œæ¨¡æ‹Ÿä¸åŒæµè§ˆå™¨
            user_agents = [
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0",
                "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.5 Safari/605.1.15",
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0",
                "Mozilla/5.0 (iPhone; CPU iPhone OS 17_0_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1"
            ]
            selected_ua = random.choice(user_agents)
            
            # æ„å»ºæ›´çœŸå®çš„è¯·æ±‚å¤´
            headers = {
                "User-Agent": selected_ua,
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
                "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
                "Accept-Encoding": "gzip, deflate, br",
                "Connection": "keep-alive",
                "Upgrade-Insecure-Requests": "1",
                "Sec-Fetch-Dest": "document",
                "Sec-Fetch-Mode": "navigate",
                "Sec-Fetch-Site": "none",
                "Sec-Fetch-User": "?1",
                "Cache-Control": "max-age=0"
            }
            
            # è®¾ç½®ä¸€ä¸ªéšæœºçš„å¼•èæ¥æºï¼Œå¾®ä¿¡æ–‡ç« æœ‰æ—¶éœ€è¦Referer
            referers = [
                "https://www.baidu.com/",
                "https://www.google.com/",
                "https://www.bing.com/",
                "https://mp.weixin.qq.com/",
                "https://weixin.qq.com/",
                "https://www.qq.com/"
            ]
            if random.random() > 0.3:  # 70%çš„æ¦‚ç‡æ·»åŠ Referer
                headers["Referer"] = random.choice(referers)
                
            # ä¸ºå¾®ä¿¡å…¬ä¼—å·æ–‡ç« æ·»åŠ ç‰¹æ®Šå¤„ç†
            if "mp.weixin.qq.com" in url:
                try:
                    # æ·»åŠ å¿…è¦çš„å¾®ä¿¡Cookieå‚æ•°ï¼Œå‡å°‘è¢«æ£€æµ‹çš„å¯èƒ½æ€§
                    cookies = {
                        "appmsglist_action_3941382959": "card",  # ä¸€äº›éšæœºçš„Cookieå€¼
                        "appmsglist_action_3941382968": "card",
                        "pac_uid": f"{int(time.time())}_f{random.randint(10000, 99999)}",
                        "rewardsn": "",
                        "wxtokenkey": f"{random.randint(100000, 999999)}",
                    }
                    
                    # ç›´æ¥ä½¿ç”¨requestsè¿›è¡Œå†…å®¹è·å–ï¼Œæœ‰æ—¶æ¯”newspaperæ›´æœ‰æ•ˆ
                    session = requests.Session()
                    response = session.get(url, headers=headers, cookies=cookies, timeout=20)
                    response.raise_for_status()
                    
                    # ä½¿ç”¨BeautifulSoupç›´æ¥è§£æ
                    from bs4 import BeautifulSoup
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # å¾®ä¿¡æ–‡ç« é€šå¸¸æœ‰è¿™äº›ç‰¹å¾
                    title_elem = soup.select_one('#activity-name')
                    author_elem = soup.select_one('#js_name') or soup.select_one('#js_profile_qrcode > div > strong')
                    content_elem = soup.select_one('#js_content')
                    
                    if content_elem:
                        # ç§»é™¤æ— ç”¨å…ƒç´ 
                        for remove_elem in content_elem.select('script, style, svg'):
                            remove_elem.extract()
                            
                        # å°è¯•è·å–æ‰€æœ‰æ–‡æœ¬
                        text_content = content_elem.get_text(separator='\n', strip=True)
                        
                        if text_content and len(text_content) > 200:  # å†…å®¹è¶³å¤Ÿé•¿
                            title = title_elem.get_text(strip=True) if title_elem else ""
                            author = author_elem.get_text(strip=True) if author_elem else "æœªçŸ¥ä½œè€…"
                            
                            # æ„å»ºå®Œæ•´å†…å®¹
                            full_content = ""
                            if title:
                                full_content += f"æ ‡é¢˜: {title}\n"
                            if author and author != "æœªçŸ¥ä½œè€…":
                                full_content += f"ä½œè€…: {author}\n"
                            full_content += f"\n{text_content}"
                            
                            logger.debug(f"[JinaSum] æˆåŠŸé€šè¿‡ç›´æ¥è¯·æ±‚æå–å¾®ä¿¡æ–‡ç« å†…å®¹ï¼Œé•¿åº¦: {len(text_content)}")
                            return full_content
                except Exception as e:
                    logger.error(f"[JinaSum] ç›´æ¥è¯·æ±‚æå–å¾®ä¿¡æ–‡ç« å¤±è´¥: {str(e)}")
                    # å¤±è´¥åä½¿ç”¨newspaperå°è¯•ï¼Œä¸è¦è¿”å›
            
            # é…ç½®newspaper
            newspaper.Config().browser_user_agent = selected_ua
            newspaper.Config().request_timeout = 30
            newspaper.Config().fetch_images = False  # ä¸ä¸‹è½½å›¾ç‰‡ä»¥åŠ å¿«é€Ÿåº¦
            newspaper.Config().memoize_articles = False  # é¿å…ç¼“å­˜å¯¼è‡´çš„é—®é¢˜
            
            # å¯¹newspaperçš„ä¸‹è½½è¿‡ç¨‹è¿›è¡Œå®šåˆ¶
            try:
                # åˆ›å»ºArticleå¯¹è±¡ä½†ä¸ç«‹å³ä¸‹è½½
                article = Article(url, language='zh')
                
                # æ‰‹åŠ¨ä¸‹è½½
                session = requests.Session()
                response = session.get(url, headers=headers, timeout=30)
                response.raise_for_status()
                
                # æ‰‹åŠ¨è®¾ç½®htmlå†…å®¹
                article.html = response.text
                article.download_state = 2  # è¡¨ç¤ºä¸‹è½½å®Œæˆ
                
                # ç„¶åè§£æ
                article.parse()
            except Exception as direct_dl_error:
                logger.error(f"[JinaSum] å°è¯•å®šåˆ¶ä¸‹è½½å¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†æ–¹æ³•: {str(direct_dl_error)}")
                article = Article(url, language='zh')
                article.download()
                article.parse()
            
            # å°è¯•è·å–å®Œæ•´å†…å®¹
            title = article.title
            authors = ', '.join(article.authors) if article.authors else "æœªçŸ¥ä½œè€…"
            publish_date = article.publish_date.strftime("%Y-%m-%d") if article.publish_date else "æœªçŸ¥æ—¥æœŸ"
            content = article.text
            
            # å¦‚æœå†…å®¹ä¸ºç©ºæˆ–è¿‡çŸ­ï¼Œå°è¯•ç›´æ¥ä»HTMLè·å–
            if not content or len(content) < 500:
                logger.debug("[JinaSum] Article content too short, trying to extract from HTML directly")
                try:
                    from bs4 import BeautifulSoup
                    soup = BeautifulSoup(article.html, 'html.parser')
                    
                    # ç§»é™¤è„šæœ¬å’Œæ ·å¼å…ƒç´ 
                    for script in soup(["script", "style"]):
                        script.extract()
                    
                    # è·å–æ‰€æœ‰æ–‡æœ¬
                    text = soup.get_text(separator='\n', strip=True)
                    
                    # å¦‚æœç›´æ¥æå–çš„å†…å®¹æ›´é•¿ï¼Œä½¿ç”¨å®ƒ
                    if len(text) > len(content):
                        content = text
                        logger.debug(f"[JinaSum] Using BeautifulSoup extracted content: {len(content)} chars")
                except Exception as bs_error:
                    logger.error(f"[JinaSum] BeautifulSoup extraction failed: {str(bs_error)}")
            
            # åˆæˆæœ€ç»ˆå†…å®¹
            if title:
                full_content = f"æ ‡é¢˜: {title}\n"
                if authors and authors != "æœªçŸ¥ä½œè€…":
                    full_content += f"ä½œè€…: {authors}\n"
                if publish_date and publish_date != "æœªçŸ¥æ—¥æœŸ":
                    full_content += f"å‘å¸ƒæ—¥æœŸ: {publish_date}\n"
                full_content += f"\n{content}"
            else:
                full_content = content
            
            if not full_content or len(full_content.strip()) < 50:
                logger.debug("[JinaSum] No content extracted by newspaper")
                
                # å°è¯•ä½¿ç”¨é€šç”¨å†…å®¹æå–æ–¹æ³•
                full_content = self._extract_content_general(url, headers)
                if full_content:
                    return full_content
                    
                return None
            
            # å¯¹äºBç«™è§†é¢‘ï¼Œå°è¯•è·å–è§†é¢‘æè¿°
            if "bilibili.com" in url or "b23.tv" in url:
                if title and not content:
                    # å¦‚æœåªæœ‰æ ‡é¢˜æ²¡æœ‰å†…å®¹ï¼Œè‡³å°‘è¿”å›æ ‡é¢˜
                    return f"æ ‡é¢˜: {title}\n\næè¿°: è¿™æ˜¯ä¸€ä¸ªBç«™è§†é¢‘ï¼Œæ— æ³•è·å–å®Œæ•´å†…å®¹ã€‚è¯·ç›´æ¥è§‚çœ‹è§†é¢‘ã€‚"
            
            logger.debug(f"[JinaSum] Successfully extracted content via newspaper, length: {len(full_content)}")
            return full_content
            
        except Exception as e:
            logger.error(f"[JinaSum] Error extracting content via newspaper: {str(e)}")
            
            # å°è¯•ä½¿ç”¨é€šç”¨å†…å®¹æå–æ–¹æ³•ä½œä¸ºå¤‡ç”¨
            try:
                logger.debug(f"[JinaSum] å°è¯•ä½¿ç”¨é€šç”¨å†…å®¹æå–æ–¹æ³•")
                content = self._extract_content_general(url)
                if content:
                    return content
            except Exception as general_error:
                logger.error(f"[JinaSum] é€šç”¨å†…å®¹æå–ä¹Ÿå¤±è´¥: {str(general_error)}")
            
            if "mp.weixin.qq.com" in url:
                return f"æ— æ³•è·å–å¾®ä¿¡å…¬ä¼—å·æ–‡ç« å†…å®¹ã€‚å¯èƒ½åŸå› ï¼š\n1. æ–‡ç« éœ€è¦ç™»å½•æ‰èƒ½æŸ¥çœ‹\n2. æ–‡ç« å·²è¢«åˆ é™¤\n3. æœåŠ¡å™¨è¢«å¾®ä¿¡é£æ§\n\nè¯·å°è¯•ç›´æ¥æ‰“å¼€é“¾æ¥: {url}"
            return None

    def _extract_content_general(self, url, headers=None):
        """é€šç”¨ç½‘é¡µå†…å®¹æå–æ–¹æ³•ï¼Œæ”¯æŒé™æ€å’ŒåŠ¨æ€é¡µé¢
        
        é¦–å…ˆå°è¯•é™æ€æå–ï¼ˆæ›´å¿«ã€æ›´è½»é‡ï¼‰ï¼Œå¦‚æœå¤±è´¥æˆ–å†…å®¹å¤ªå°‘å†å°è¯•åŠ¨æ€æå–ï¼ˆæ›´æ…¢ä½†æ›´å¼ºå¤§ï¼‰
        
        Args:
            url: ç½‘é¡µURL
            headers: å¯é€‰çš„è¯·æ±‚å¤´ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤
            
        Returns:
            str: æå–çš„å†…å®¹ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            import random
            from bs4 import BeautifulSoup
            
            # å¦‚æœæ˜¯ç™¾åº¦æ–‡ç« é“¾æ¥ï¼Œä½¿ç”¨ä¸“é—¨çš„å¤„ç†æ–¹æ³•
            if "md.mbd.baidu.com" in url or "mbd.baidu.com" in url:
                # ç›´æ¥ä½¿ç”¨ä¸“é—¨çš„ç™¾åº¦æ–‡ç« æå–æ–¹æ³•
                content = self._extract_baidu_article(url)
                if content:
                    return content
            
            # å¦‚æœæ²¡æœ‰æä¾›headersï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤çš„
            if not headers:
                headers = self._get_default_headers()
            
            # æ·»åŠ éšæœºå»¶è¿Ÿä»¥é¿å…è¢«æ£€æµ‹ä¸ºçˆ¬è™«
            time.sleep(random.uniform(0.5, 2))
            
            # åˆ›å»ºä¼šè¯å¯¹è±¡
            session = requests.Session()
            
            # è®¾ç½®åŸºæœ¬cookies
            session.cookies.update({
                f"visit_id_{int(time.time())}": f"{random.randint(1000000, 9999999)}",
                "has_visited": "1",
            })
            
            # å‘é€è¯·æ±‚è·å–é¡µé¢
            logger.debug(f"[JinaSum] é€šç”¨æå–æ–¹æ³•æ­£åœ¨è¯·æ±‚: {url}")
            response = session.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            
            # ç¡®ä¿ç¼–ç æ­£ç¡®
            if response.encoding == 'ISO-8859-1':
                response.encoding = response.apparent_encoding
                
            # ä½¿ç”¨BeautifulSoupè§£æHTML
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ç§»é™¤æ— ç”¨å…ƒç´ 
            for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'form', 'iframe']):
                element.extract()
            
            # å¯»æ‰¾å¯èƒ½çš„æ ‡é¢˜
            title = None
            
            # å°è¯•å¤šç§æ ‡é¢˜é€‰æ‹©å™¨
            title_candidates = [
                soup.select_one('h1'),  # æœ€å¸¸è§çš„æ ‡é¢˜æ ‡ç­¾
                soup.select_one('title'),  # HTMLæ ‡é¢˜
                soup.select_one('.title'),  # å¸¸è§çš„æ ‡é¢˜ç±»
                soup.select_one('.article-title'),  # å¸¸è§çš„æ–‡ç« æ ‡é¢˜ç±»
                soup.select_one('.post-title'),  # åšå®¢æ ‡é¢˜
                soup.select_one('[class*="title" i]'),  # åŒ…å«titleçš„ç±»
            ]
            
            for candidate in title_candidates:
                if candidate and candidate.text.strip():
                    title = candidate.text.strip()
                    break
            
            # æŸ¥æ‰¾å¯èƒ½çš„å†…å®¹å…ƒç´ 
            content_candidates = []
            
            # 1. å°è¯•æ‰¾å¸¸è§çš„å†…å®¹å®¹å™¨
            content_selectors = [
                'article', 'main', '.content', '.article', '.post-content',
                '[class*="content" i]', '[class*="article" i]', 
                '.story', '.entry-content', '.post-body',
                '#content', '#article', '.body'
            ]
            
            for selector in content_selectors:
                elements = soup.select(selector)
                if elements:
                    content_candidates.extend(elements)
            
            # 2. å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ç¡®çš„å†…å®¹å®¹å™¨ï¼Œå¯»æ‰¾å…·æœ‰æœ€å¤šæ–‡æœ¬çš„divå…ƒç´ 
            if not content_candidates:
                paragraphs = {}
                # æŸ¥æ‰¾æ‰€æœ‰æ®µè½å’Œdiv
                for elem in soup.find_all(['p', 'div']):
                    text = elem.get_text(strip=True)
                    # åªè€ƒè™‘æœ‰å®é™…å†…å®¹çš„å…ƒç´ 
                    if len(text) > 100:
                        paragraphs[elem] = len(text)
                
                # æ‰¾å‡ºæ–‡æœ¬æœ€å¤šçš„å…ƒç´ 
                if paragraphs:
                    max_elem = max(paragraphs.items(), key=lambda x: x[1])[0]
                    # å¦‚æœæ˜¯divï¼Œç›´æ¥æ·»åŠ ï¼›å¦‚æœæ˜¯pï¼Œå°è¯•æ‰¾å…¶çˆ¶å…ƒç´ 
                    if max_elem.name == 'div':
                        content_candidates.append(max_elem)
                    else:
                        # æ‰¾åŒ…å«å¤šä¸ªæ®µè½çš„çˆ¶å…ƒç´ 
                        parent = max_elem.parent
                        if parent and len(parent.find_all('p')) > 3:
                            content_candidates.append(parent)
                        else:
                            content_candidates.append(max_elem)
            
            # 3. ç®€å•ç®—æ³•æ¥è¯„åˆ†å’Œé€‰æ‹©æœ€ä½³å†…å®¹å…ƒç´ 
            best_content = None
            max_score = 0
            
            for element in content_candidates:
                # è®¡ç®—æ–‡æœ¬é•¿åº¦
                text = element.get_text(strip=True)
                text_length = len(text)
                
                # è®¡ç®—æ–‡æœ¬å¯†åº¦ï¼ˆæ–‡æœ¬é•¿åº¦/HTMLé•¿åº¦ï¼‰
                html_length = len(str(element))
                text_density = text_length / html_length if html_length > 0 else 0
                
                # è®¡ç®—æ®µè½æ•°é‡
                paragraphs = element.find_all('p')
                paragraph_count = len(paragraphs)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å›¾ç‰‡
                images = element.find_all('img')
                image_count = len(images)
                
                # æ ¹æ®å„ç§ç‰¹å¾è®¡ç®—åˆ†æ•°
                score = (
                    text_length * 1.0 +  # æ–‡æœ¬é•¿åº¦å¾ˆé‡è¦
                    text_density * 100 +  # æ–‡æœ¬å¯†åº¦å¾ˆé‡è¦
                    paragraph_count * 30 +  # æ®µè½æ•°é‡ä¹Ÿå¾ˆé‡è¦
                    image_count * 10  # å›¾ç‰‡ä¸å¤ªé‡è¦ï¼Œä½†ä¹Ÿæ˜¯ä¸€ä¸ªæŒ‡æ ‡
                )
                
                # å‡åˆ†é¡¹ï¼šå¦‚æœåŒ…å«è®¸å¤šé“¾æ¥ï¼Œå¯èƒ½æ˜¯å¯¼èˆªæˆ–ä¾§è¾¹æ 
                links = element.find_all('a')
                link_text_ratio = sum(len(a.get_text(strip=True)) for a in links) / text_length if text_length > 0 else 0
                if link_text_ratio > 0.5:  # å¦‚æœé“¾æ¥æ–‡æœ¬å æ¯”è¿‡é«˜
                    score *= 0.5
                
                # æ›´æ–°æœ€ä½³å†…å®¹
                if score > max_score:
                    max_score = score
                    best_content = element
            
            # å¦‚æœæ‰¾åˆ°å†…å®¹ï¼Œæå–å¹¶æ¸…ç†æ–‡æœ¬
            static_content_result = None
            if best_content:
                # é¦–å…ˆç§»é™¤å†…å®¹ä¸­å¯èƒ½çš„å¹¿å‘Šæˆ–æ— å…³å…ƒç´ 
                for ad in best_content.select('[class*="ad" i], [class*="banner" i], [id*="ad" i], [class*="recommend" i]'):
                    ad.extract()
                
                # è·å–å¹¶æ¸…ç†æ–‡æœ¬
                content_text = best_content.get_text(separator='\n', strip=True)
                
                # ç§»é™¤å¤šä½™çš„ç©ºç™½è¡Œ
                content_text = re.sub(r'\n{3,}', '\n\n', content_text)
                
                # æ„å»ºæœ€ç»ˆè¾“å‡º
                result = ""
                if title:
                    result += f"æ ‡é¢˜: {title}\n\n"
                
                result += content_text
                
                logger.debug(f"[JinaSum] é€šç”¨æå–æ–¹æ³•æˆåŠŸï¼Œæå–å†…å®¹é•¿åº¦: {len(result)}")
                static_content_result = result
            
            # åˆ¤æ–­é™æ€æå–çš„å†…å®¹è´¨é‡
            content_is_good = False
            if static_content_result:
                # å†…å®¹é•¿åº¦æ£€æŸ¥
                if len(static_content_result) > 1000:
                    content_is_good = True
                # ç»“æ„æ£€æŸ¥ - è‡³å°‘åº”è¯¥æœ‰å¤šä¸ªæ®µè½
                elif static_content_result.count('\n\n') >= 3:
                    content_is_good = True
            
            # å¦‚æœé™æ€æå–å†…å®¹è´¨é‡ä¸ä½³ï¼Œå°è¯•åŠ¨æ€æå–
            if not content_is_good:
                logger.debug("[JinaSum] é™æ€æå–å†…å®¹è´¨é‡ä¸ä½³ï¼Œå°è¯•åŠ¨æ€æå–")
                dynamic_content = self._extract_dynamic_content(url, headers)
                if dynamic_content:
                    logger.debug(f"[JinaSum] åŠ¨æ€æå–æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(dynamic_content)}")
                    return dynamic_content
            
            return static_content_result
                
        except Exception as e:
            logger.error(f"[JinaSum] é€šç”¨å†…å®¹æå–æ–¹æ³•å¤±è´¥: {str(e)}", exc_info=True)
            return None

    def _extract_dynamic_content(self, url, headers=None):
        """ä½¿ç”¨JavaScriptæ¸²æŸ“æå–åŠ¨æ€é¡µé¢å†…å®¹
        
        Args:
            url: ç½‘é¡µURL
            headers: å¯é€‰çš„è¯·æ±‚å¤´
            
        Returns:
            str: æå–çš„å†…å®¹ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            from requests_html import HTMLSession
            from bs4 import BeautifulSoup
            
            logger.debug(f"[JinaSum] å¼€å§‹åŠ¨æ€æå–å†…å®¹: {url}")
            
            # åˆ›å»ºä¼šè¯å¹¶è®¾ç½®è¶…æ—¶
            session = HTMLSession()
            
            # æ·»åŠ è¯·æ±‚å¤´
            req_headers = headers or self._get_default_headers()
            
            # è·å–é¡µé¢
            response = session.get(url, headers=req_headers, timeout=30)
            
            # æ‰§è¡ŒJavaScript (è®¾ç½®è¶…æ—¶ï¼Œé˜²æ­¢æ— é™ç­‰å¾…)
            logger.debug("[JinaSum] å¼€å§‹æ‰§è¡ŒJavaScript")
            response.html.render(timeout=20, sleep=2)
            logger.debug("[JinaSum] JavaScriptæ‰§è¡Œå®Œæˆ")
            
            # å¤„ç†æ¸²æŸ“åçš„HTML
            rendered_html = response.html.html
            
            # ä½¿ç”¨BeautifulSoupè§£ææ¸²æŸ“åçš„HTML
            soup = BeautifulSoup(rendered_html, 'html.parser')
            
            # æ¸…ç†æ— ç”¨å…ƒç´ 
            for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
                element.extract()
            
            # æŸ¥æ‰¾æ ‡é¢˜
            title = None
            title_candidates = [
                soup.select_one('h1'),
                soup.select_one('title'),
                soup.select_one('.title'),
                soup.select_one('[class*="title" i]'),
            ]
            
            for candidate in title_candidates:
                if candidate and candidate.text.strip():
                    title = candidate.text.strip()
                    break
            
            # å¯»æ‰¾ä¸»è¦å†…å®¹
            main_content = None
            
            # 1. å°è¯•æ‰¾ä¸»è¦å†…å®¹å®¹å™¨
            main_selectors = [
                'article', 'main', '.content', '.article',
                '[class*="content" i]', '[class*="article" i]',
                '#content', '#article'
            ]
            
            for selector in main_selectors:
                elements = soup.select(selector)
                if elements:
                    # é€‰æ‹©åŒ…å«æœ€å¤šæ–‡æœ¬çš„å…ƒç´ 
                    main_content = max(elements, key=lambda x: len(x.get_text()))
                    break
            
            # 2. å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå¯»æ‰¾æ–‡æœ¬æœ€å¤šçš„div
            if not main_content:
                paragraphs = {}
                for elem in soup.find_all(['div']):
                    text = elem.get_text(strip=True)
                    if len(text) > 200:  # åªè€ƒè™‘é•¿æ–‡æœ¬
                        paragraphs[elem] = len(text)
                
                if paragraphs:
                    main_content = max(paragraphs.items(), key=lambda x: x[1])[0]
            
            # 3. å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨æ•´ä¸ªbody
            if not main_content:
                main_content = soup.body
            
            # ä»ä¸»è¦å†…å®¹ä¸­æå–æ–‡æœ¬
            if main_content:
                # æ¸…ç†å¯èƒ½çš„å¹¿å‘Šæˆ–æ— å…³å…ƒç´ 
                for ad in main_content.select('[class*="ad" i], [class*="banner" i], [id*="ad" i], [class*="recommend" i]'):
                    ad.extract()
                
                # è·å–æ–‡æœ¬
                content_text = main_content.get_text(separator='\n', strip=True)
                content_text = re.sub(r'\n{3,}', '\n\n', content_text)  # æ¸…ç†å¤šä½™ç©ºè¡Œ
                
                # æ„å»ºæœ€ç»ˆç»“æœ
                result = ""
                if title:
                    result += f"æ ‡é¢˜: {title}\n\n"
                result += content_text
                
                # å…³é—­ä¼šè¯
                session.close()
                
                return result
            
            # å…³é—­ä¼šè¯
            session.close()
            
            return None
            
        except Exception as e:
            logger.error(f"[JinaSum] åŠ¨æ€æå–å¤±è´¥: {str(e)}", exc_info=True)
            return None

    def _extract_baidu_article(self, url):
        """ä¸“é—¨ç”¨äºæå–ç™¾åº¦æ–‡ç« å†…å®¹çš„æ–¹æ³•
        
        Args:
            url: ç™¾åº¦æ–‡ç« URL
            
        Returns:
            str: æå–çš„å†…å®¹ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            import random
            import json
            from bs4 import BeautifulSoup
            
            logger.debug(f"[JinaSum] å°è¯•ä¸“é—¨æå–ç™¾åº¦æ–‡ç« : {url}")
            
            # æå–æ–‡ç« ID
            article_id = None
            parsed_url = urlparse(url)
            path_parts = parsed_url.path.split('/')
            
            # ä¾‹å¦‚ /r/1A1GKWoodMI
            if len(path_parts) > 1 and path_parts[-2] == 'r':
                article_id = path_parts[-1]
            
            # ä¾‹å¦‚ ?r=1A1GKWoodMI
            if not article_id:
                query_params = parse_qs(parsed_url.query)
                if 'r' in query_params:
                    article_id = query_params['r'][0]
            
            if not article_id:
                logger.error(f"[JinaSum] æ— æ³•ä»URLæå–ç™¾åº¦æ–‡ç« ID: {url}")
                return None
                
            logger.debug(f"[JinaSum] æå–åˆ°ç™¾åº¦æ–‡ç« ID: {article_id}")
            
            # æ„å»ºå¤šç§URLå°è¯•æå–
            url_formats = [
                # å°è¯•ç›´æ¥è®¿é—®åŸå§‹URL
                url,
                # å°è¯•ç§»åŠ¨ç½‘é¡µç‰ˆæ ¼å¼1
                f"https://mbd.baidu.com/newspage/data/landingshare?context=%7B%22nid%22%3A%22news_{article_id}%22%2C%22sourceFrom%22%3A%22bjh%22%7D",
                # å°è¯•ç§»åŠ¨ç½‘é¡µç‰ˆæ ¼å¼2
                f"https://mbd.baidu.com/newspage/data/landingsuper?context=%7B%22nid%22%3A%22news_{article_id}%22%7D"
            ]
            
            # ä½¿ç”¨ç§»åŠ¨è®¾å¤‡UA
            mobile_user_agents = [
                "Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Mobile/15E148 Safari/604.1",
                "Mozilla/5.0 (Linux; Android 11; Pixel 5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.91 Mobile Safari/537.36",
                "Mozilla/5.0 (iPad; CPU OS 15_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) CriOS/94.0.4606.76 Mobile/15E148 Safari/604.1"
            ]
            
            # å°è¯•æ¯ç§URLæ ¼å¼
            for target_url in url_formats:
                try:
                    logger.debug(f"[JinaSum] å°è¯•ç™¾åº¦æ–‡ç« URLæ ¼å¼: {target_url}")
                    
                    # æ„å»ºè¯·æ±‚å¤´
                    headers = {
                        "User-Agent": random.choice(mobile_user_agents),
                        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
                        "Connection": "keep-alive",
                        "Cache-Control": "no-cache",
                        "Pragma": "no-cache"
                    }
                    
                    # å‘é€è¯·æ±‚
                    response = requests.get(
                        target_url, 
                        headers=headers, 
                        timeout=15,
                        allow_redirects=True
                    )
                    response.raise_for_status()
                    
                    # ç¡®ä¿ç¼–ç æ­£ç¡®
                    if response.encoding == 'ISO-8859-1':
                        response.encoding = response.apparent_encoding
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯JSONå“åº” - æŸäº›ç™¾åº¦APIä¼šè¿”å›JSON
                    content_type = response.headers.get('Content-Type', '')
                    if 'application/json' in content_type or response.text.strip().startswith('{'):
                        try:
                            data = json.loads(response.text)
                            # æ£€æŸ¥JSONæ•°æ®ä¸­æ˜¯å¦åŒ…å«æ–‡ç« å†…å®¹
                            if data.get('data', {}).get('title') and (data.get('data', {}).get('content') or data.get('data', {}).get('html')):
                                title = data['data']['title']
                                content_html = data['data'].get('content', '') or data['data'].get('html', '')
                                author = data['data'].get('author', '')
                                publish_time = data['data'].get('publish_time', '')
                                
                                # è§£æHTMLå†…å®¹
                                content_soup = BeautifulSoup(content_html, 'html.parser')
                                
                                # ç§»é™¤è„šæœ¬å’Œæ ·å¼
                                for tag in content_soup(['script', 'style']):
                                    tag.decompose()
                                
                                # æå–çº¯æ–‡æœ¬
                                content_text = content_soup.get_text(separator='\n', strip=True)
                                
                                # æ„å»ºç»“æœ
                                result = f"æ ‡é¢˜: {title}\n"
                                if author:
                                    result += f"ä½œè€…: {author}\n"
                                if publish_time:
                                    result += f"æ—¶é—´: {publish_time}\n"
                                    
                                result += f"\n{content_text}"
                                
                                logger.debug(f"[JinaSum] æˆåŠŸé€šè¿‡JSONæå–ç™¾åº¦æ–‡ç« ï¼Œé•¿åº¦: {len(result)}")
                                return result
                        except json.JSONDecodeError:
                            # ä¸æ˜¯JSONï¼Œç»§ç»­å½“ä½œHTMLå¤„ç†
                            pass
                    
                    # è§£æHTML
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # å…ˆå°è¯•æå–å¯èƒ½çš„JSONæ•°æ®
                    # ç™¾åº¦æœ‰æ—¶ä¼šåœ¨é¡µé¢ä¸­åµŒå…¥æ–‡ç« JSONæ•°æ®
                    for script in soup.find_all('script'):
                        script_text = script.string
                        if script_text and ('content' in script_text or 'article' in script_text):
                            try:
                                # å°è¯•æ‰¾åˆ°JSONæ ¼å¼çš„æ•°æ®
                                json_start = script_text.find('{')
                                json_end = script_text.rfind('}') + 1
                                if json_start >= 0 and json_end > json_start:
                                    json_str = script_text[json_start:json_end]
                                    data = json.loads(json_str)
                                    
                                    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ–‡ç« æ•°æ®
                                    article_data = None
                                    if 'article' in data:
                                        article_data = data['article']
                                    elif 'data' in data and 'article' in data['data']:
                                        article_data = data['data']['article']
                                    
                                    if article_data and 'title' in article_data:
                                        title = article_data.get('title', '')
                                        content = article_data.get('content', '')
                                        author = article_data.get('author', '')
                                        publish_time = article_data.get('publish_time', '')
                                        
                                        # è§£æHTMLå†…å®¹
                                        if content:
                                            content_soup = BeautifulSoup(content, 'html.parser')
                                            content_text = content_soup.get_text(separator='\n', strip=True)
                                            
                                            # æ„å»ºç»“æœ
                                            result = f"æ ‡é¢˜: {title}\n"
                                            if author:
                                                result += f"ä½œè€…: {author}\n"
                                            if publish_time:
                                                result += f"æ—¶é—´: {publish_time}\n"
                                                
                                            result += f"\n{content_text}"
                                            
                                            logger.debug(f"[JinaSum] æˆåŠŸä»åµŒå…¥JSONæå–ç™¾åº¦æ–‡ç« ï¼Œé•¿åº¦: {len(result)}")
                                            return result
                            except Exception as json_err:
                                logger.debug(f"[JinaSum] ä»è„šæœ¬æå–JSONå¤±è´¥: {str(json_err)}")
                    
                    # å°è¯•ä»HTMLç›´æ¥æå–å†…å®¹
                    # æå–æ ‡é¢˜
                    title = None
                    for selector in ['.article-title', '.title', 'h1.title', 'h1']:
                        title_elem = soup.select_one(selector)
                        if title_elem and title_elem.text.strip():
                            title = title_elem.text.strip()
                            break
                    
                    # å¦‚æœæ²¡æ‰¾åˆ°æ ‡é¢˜ï¼Œå°è¯•ä½¿ç”¨æ ‡é¢˜æ ‡ç­¾
                    if not title:
                        title_tag = soup.find('title')
                        if title_tag:
                            title = title_tag.text.strip()
                    
                    # æå–ä½œè€…
                    author = None
                    for selector in ['.author', '.writer', '.source', '.article-author']:
                        author_elem = soup.select_one(selector)
                        if author_elem and author_elem.text.strip():
                            author = author_elem.text.strip()
                            break
                    
                    # æå–å†…å®¹
                    content = None
                    for selector in ['.article-content', '.article-detail', '.content', '.artcle', '#article']:
                        content_elem = soup.select_one(selector)
                        if content_elem:
                            # ç§»é™¤æ— ç”¨å…ƒç´ 
                            for remove_elem in content_elem.select('.ad-banner, .recommend, .share-btn, script, style'):
                                remove_elem.extract()
                            
                            content_text = content_elem.get_text(separator='\n', strip=True)
                            if len(content_text) > 200:  # å†…å®¹è¶³å¤Ÿé•¿
                                content = content_text
                                break
                    
                    # å¦‚æœæ²¡æ‰¾åˆ°å†…å®¹ï¼Œå°è¯•æŸ¥æ‰¾æœ€é•¿çš„æ®µè½é›†åˆ
                    if not content:
                        max_paragraphs = []
                        max_text_len = 0
                        
                        # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„å†…å®¹å®¹å™¨
                        for div in soup.find_all('div'):
                            paragraphs = div.find_all('p')
                            if len(paragraphs) >= 3:  # è‡³å°‘æœ‰3ä¸ªæ®µè½
                                text = '\n'.join([p.get_text(strip=True) for p in paragraphs])
                                if len(text) > max_text_len:
                                    max_text_len = len(text)
                                    max_paragraphs = paragraphs
                        
                        # å¦‚æœæ‰¾åˆ°è¶³å¤Ÿé•¿çš„æ®µè½é›†åˆ
                        if max_text_len > 200:
                            content = '\n'.join([p.get_text(strip=True) for p in max_paragraphs])
                    
                    # å¦‚æœæ‰¾åˆ°å†…å®¹ï¼Œæ„å»ºç»“æœ
                    if content:
                        result = ""
                        if title:
                            result += f"æ ‡é¢˜: {title}\n"
                        if author:
                            result += f"ä½œè€…: {author}\n"
                        result += f"\n{content}"
                        
                        logger.debug(f"[JinaSum] æˆåŠŸé€šè¿‡HTMLæå–ç™¾åº¦æ–‡ç« ï¼Œé•¿åº¦: {len(result)}")
                        return result
                
                except Exception as e:
                    logger.debug(f"[JinaSum] å°è¯•URL {target_url} å¤±è´¥: {str(e)}")
                    continue  # å°è¯•ä¸‹ä¸€ä¸ªURLæ ¼å¼
            
            # æ‰€æœ‰å°è¯•éƒ½å¤±è´¥ï¼Œè¿”å›None
            logger.error(f"[JinaSum] æ‰€æœ‰ç™¾åº¦æ–‡ç« æå–æ–¹æ³•å‡å¤±è´¥")
            return None
            
        except Exception as e:
            logger.error(f"[JinaSum] ä¸“é—¨æå–ç™¾åº¦æ–‡ç« å¤±è´¥: {str(e)}")
            return None

    def _get_default_headers(self):
        """è·å–é»˜è®¤è¯·æ±‚å¤´"""
        import random
        
        user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.5 Safari/605.1.15",
        ]
        selected_ua = random.choice(user_agents)
        
        return {
            "User-Agent": selected_ua,
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Cache-Control": "max-age=0",
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "none",
            "Sec-Fetch-User": "?1"
        }

    def _process_summary(self, content: str, e_context: EventContext, retry_count: int = 0, skip_notice: bool = False):
        """å¤„ç†æ€»ç»“è¯·æ±‚"""
        try:
            if not self._check_url(content):
                logger.debug(f"[JinaSum] {content} is not a valid url, skip")
                return
                
            if retry_count == 0 and not skip_notice:
                logger.debug("[JinaSum] Processing URL: %s" % content)
                reply = Reply(ReplyType.TEXT, "ğŸ‰æ­£åœ¨ä¸ºæ‚¨ç”Ÿæˆæ€»ç»“ï¼Œè¯·ç¨å€™...")
                channel = e_context["channel"]
                channel.send(reply, e_context["context"])

            # è·å–ç½‘é¡µå†…å®¹
            target_url = html.unescape(content)
            target_url_content = None
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«XMLæ•°æ®ï¼ˆåˆ†äº«æ¶ˆæ¯é”™è¯¯ï¼‰
            if target_url.startswith("<") and "appmsg" in target_url:
                logger.warning("[JinaSum] æ£€æµ‹åˆ°XMLæ•°æ®è€Œä¸æ˜¯URLï¼Œå°è¯•æå–çœŸå®URL")
                try:
                    import xml.etree.ElementTree as ET
                    # å¤„ç†å¯èƒ½çš„XMLå£°æ˜
                    if target_url.startswith('<?xml'):
                        target_url = target_url[target_url.find('<msg>'):]
                    
                    root = ET.fromstring(target_url)
                    url_elem = root.find(".//url")
                    if url_elem is not None and url_elem.text:
                        target_url = url_elem.text
                        logger.debug(f"[JinaSum] ä»XMLä¸­æå–åˆ°URL: {target_url}")
                    else:
                        logger.error("[JinaSum] æ— æ³•ä»XMLä¸­æå–URL")
                        raise ValueError("æ— æ³•ä»åˆ†äº«å¡ç‰‡ä¸­æå–URL")
                except Exception as ex:
                    logger.error(f"[JinaSum] è§£æXMLå¤±è´¥: {str(ex)}")
                    raise ValueError("æ— æ³•ä»åˆ†äº«å¡ç‰‡ä¸­æå–URL")
            
            # ä½¿ç”¨newspaper3kæå–å†…å®¹
            logger.debug(f"[JinaSum] ä½¿ç”¨newspaper3kæå–å†…å®¹: {target_url}")
            target_url_content = self._get_content_via_newspaper(target_url)
            
            # æ£€æŸ¥è¿”å›çš„å†…å®¹æ˜¯å¦åŒ…å«éªŒè¯æç¤º
            if target_url_content and target_url_content.startswith("âš ï¸"):
                # è¿™æ˜¯ä¸€ä¸ªéªŒè¯æç¤ºï¼Œç›´æ¥è¿”å›ç»™ç”¨æˆ·
                logger.info(f"[JinaSum] è¿”å›éªŒè¯æç¤ºç»™ç”¨æˆ·: {target_url_content}")
                reply = Reply(ReplyType.INFO, target_url_content)
                e_context["reply"] = reply
                e_context.action = EventAction.BREAK_PASS
                return
            
            # å¦‚æœnewspaperæå–å¤±è´¥ï¼Œç›´æ¥ä½¿ç”¨é€šç”¨æå–æ–¹æ³•
            if not target_url_content:
                logger.debug(f"[JinaSum] newspaperæå–å¤±è´¥ï¼Œç›´æ¥ä½¿ç”¨é€šç”¨æå–æ–¹æ³•: {target_url}")
                target_url_content = self._extract_content_general(target_url)
            
            # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥
            if not target_url_content:
                # å¯¹äºBç«™è§†é¢‘ï¼Œæä¾›ç‰¹æ®Šå¤„ç†
                if "bilibili.com" in target_url or "b23.tv" in target_url:
                    target_url_content = "è¿™æ˜¯ä¸€ä¸ªBç«™è§†é¢‘é“¾æ¥ã€‚ç”±äºè§†é¢‘å†…å®¹æ— æ³•ç›´æ¥æå–ï¼Œè¯·ç›´æ¥ç‚¹å‡»é“¾æ¥è§‚çœ‹è§†é¢‘ã€‚"
                else:
                    raise ValueError("æ— æ³•æå–æ–‡ç« å†…å®¹")
                
            # æ¸…æ´—å†…å®¹
            target_url_content = self._clean_content(target_url_content)
            
            # é™åˆ¶å†…å®¹é•¿åº¦
            target_url_content = target_url_content[:self.max_words]
            logger.debug(f"[JinaSum] Got content length: {len(target_url_content)}")
            
            # æ„é€ æç¤ºè¯å’Œå†…å®¹
            sum_prompt = f"{self.prompt}\n\n'''{target_url_content}'''"
            
            # ä¿®æ”¹contextå†…å®¹ï¼Œä½¿ç”¨ä¼ é€’å¼æ¶ˆæ¯
            e_context['context'].type = ContextType.TEXT
            e_context['context'].content = sum_prompt
            e_context.action = EventAction.CONTINUE
            logger.debug("[JinaSum] ä½¿ç”¨ä¼ é€’å¼æ¶ˆæ¯å¤„ç†")
            return
                
        except Exception as e:
            logger.error(f"[JinaSum] Error in processing summary: {str(e)}")
            
            if retry_count < 3:
                logger.info(f"[JinaSum] Retrying {retry_count + 1}/3...")
                return self._process_summary(content, e_context, retry_count + 1, True)
            
            error_msg = "æŠ±æ­‰ï¼Œæ— æ³•è·å–æ–‡ç« å†…å®¹ã€‚å¯èƒ½æ˜¯å› ä¸º:\n"
            error_msg += "1. æ–‡ç« éœ€è¦ç™»å½•æˆ–å·²è¿‡æœŸ\n"
            error_msg += "2. æ–‡ç« æœ‰ç‰¹æ®Šçš„è®¿é—®é™åˆ¶\n"
            error_msg += "3. ç½‘ç»œè¿æ¥ä¸ç¨³å®š\n\n"
            error_msg += "å»ºè®®æ‚¨:\n"
            error_msg += "- ç›´æ¥æ‰“å¼€é“¾æ¥æŸ¥çœ‹\n"
            error_msg += "- ç¨åé‡è¯•\n"
            error_msg += "- å°è¯•å…¶ä»–æ–‡ç« "
            
            reply = Reply(ReplyType.ERROR, error_msg)
            e_context["reply"] = reply
            e_context.action = EventAction.BREAK_PASS

    def get_help_text(self, verbose, **kwargs):
        help_text = "ç½‘é¡µå†…å®¹æ€»ç»“æ’ä»¶:\n"
        help_text += "1. å‘é€ã€Œæ€»ç»“ ç½‘å€ã€å¯ä»¥æ€»ç»“æŒ‡å®šç½‘é¡µçš„å†…å®¹\n"
        help_text += "2. å•èŠæ—¶åˆ†äº«æ¶ˆæ¯ä¼šè‡ªåŠ¨æ€»ç»“\n"
        if self.auto_sum:
            help_text += "3. ç¾¤èŠä¸­åˆ†äº«æ¶ˆæ¯é»˜è®¤è‡ªåŠ¨æ€»ç»“"
            if self.black_group_list:
                help_text += "ï¼ˆéƒ¨åˆ†ç¾¤ç»„éœ€è¦å‘é€å«ã€Œæ€»ç»“ã€çš„æ¶ˆæ¯è§¦å‘ï¼‰\n"
            else:
                help_text += "\n"
        else:
            help_text += "3. ç¾¤èŠä¸­æ”¶åˆ°åˆ†äº«æ¶ˆæ¯åï¼Œå‘é€åŒ…å«ã€Œæ€»ç»“ã€çš„æ¶ˆæ¯å³å¯è§¦å‘æ€»ç»“\n"
        help_text += "æ³¨ï¼šç¾¤èŠä¸­çš„åˆ†äº«æ¶ˆæ¯çš„æ€»ç»“è¯·æ±‚éœ€è¦åœ¨60ç§’å†…å‘å‡º"
        return help_text

    def _load_config_template(self):
        """åŠ è½½é…ç½®æ¨¡æ¿"""
        try:
            template_path = os.path.join(os.path.dirname(__file__), "config.json.template")
            if os.path.exists(template_path):
                with open(template_path, "r", encoding="utf-8") as f:
                    plugin_conf = json.load(f)
                    return plugin_conf
        except Exception as e:
            logger.exception(e)

    def _get_openai_chat_url(self):
        return self.open_ai_api_base + "/chat/completions"

    def _get_openai_headers(self):
        """è·å–openaiçš„header"""
        config = super().get_config()
        return {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {config.get('openai_api_key')}"
        }

    def _get_openai_payload(self, target_url_content):
        """æ„é€ openaiçš„payload
        
        Args:
            target_url_content: ç½‘é¡µå†…å®¹
        """
        sum_prompt = f"{self.prompt}\n\n'''{target_url_content}'''"
        messages = [{"role": "user", "content": sum_prompt}]
        payload = {
            'model': self.open_ai_model,
            'messages': messages
        }
        return payload

    def _check_url(self, target_url: str):
        """æ£€æŸ¥URLæ˜¯å¦æœ‰æ•ˆä¸”å…è®¸è®¿é—®
        
        Args:
            target_url: è¦æ£€æŸ¥çš„URL
            
        Returns:
            bool: URLæ˜¯å¦æœ‰æ•ˆä¸”å…è®¸è®¿é—®
        """
        stripped_url = target_url.strip()
        logger.debug(f"[JinaSum] æ£€æŸ¥URL: {stripped_url}")
        
        # ç®€å•æ ¡éªŒæ˜¯å¦æ˜¯url
        if not stripped_url.startswith("http://") and not stripped_url.startswith("https://"):
            logger.debug("[JinaSum] URLä¸ä»¥http://æˆ–https://å¼€å¤´ï¼Œè·³è¿‡")
            return False

        # æ£€æµ‹ä¸€äº›å¸¸è§çš„ä¸é€‚åˆæ€»ç»“çš„å†…å®¹ç±»å‹
        skip_patterns = [
            # è§†é¢‘/éŸ³ä¹å¹³å°çš„éæ–‡ç« å†…å®¹
            r"(bilibili\.com|b23\.tv).*/video/", # Bç«™è§†é¢‘
            r"(youtube\.com|youtu\.be)/watch", # YouTubeè§†é¢‘
            r"(music\.163\.com|y\.qq\.com)/(song|playlist|album)", # éŸ³ä¹
            
            # æ–‡ä»¶é“¾æ¥
            r"\.(pdf|doc|docx|ppt|pptx|xls|xlsx|zip|rar|7z)(\?|$)", # æ–‡æ¡£å’Œå‹ç¼©åŒ…
            
            # å›¾ç‰‡é“¾æ¥
            r"\.(jpg|jpeg|png|gif|bmp|webp|svg)(\?|$)", # å›¾ç‰‡
            
            # åœ°å›¾
            r"(map\.(baidu|google|qq)\.com)", # åœ°å›¾
            
            # å·¥å…·ç±»
            r"(docs\.qq\.com|shimo\.im|yuque\.com|notion\.so)", # åœ¨çº¿æ–‡æ¡£
            
            # ç¤¾äº¤åª’ä½“ç‰¹å®šå†…å®¹
            r"weixin\.qq\.com/[^/]+/([^/]+/){2,}",  # å¾®ä¿¡å°ç¨‹åºæˆ–å…¶ä»–åŠŸèƒ½
            r"(weibo\.com|t\.cn)/[^/]+/[^/]+",  # å¾®åš
            
            # å•†åŸå•†å“
            r"(taobao\.com|tmall\.com|jd\.com)/.*?(item|product)",  # ç”µå•†å•†å“
            
            # å°ç¨‹åº
            r"servicewechat\.com"  # å¾®ä¿¡å°ç¨‹åº
        ]
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ£€æŸ¥
        import re
        for pattern in skip_patterns:
            if re.search(pattern, stripped_url, re.IGNORECASE):
                logger.debug(f"[JinaSum] URLåŒ¹é…è·³è¿‡æ¨¡å¼: {pattern}")
                return False

        # æ£€æŸ¥ç™½åå•
        if len(self.white_url_list):
            if not any(stripped_url.startswith(white_url) for white_url in self.white_url_list):
                logger.debug("[JinaSum] URLä¸åœ¨ç™½åå•ä¸­")
                return False

        # æ’é™¤é»‘åå•ï¼Œé»‘åå•ä¼˜å…ˆçº§>ç™½åå•
        for black_url in self.black_url_list:
            if stripped_url.startswith(black_url):
                logger.debug(f"[JinaSum] URLåœ¨é»‘åå•ä¸­: {black_url}")
                return False

        logger.debug("[JinaSum] URLæ£€æŸ¥é€šè¿‡")
        return True

    def _clean_content(self, content: str) -> str:
        """æ¸…æ´—å†…å®¹ï¼Œå»é™¤å›¾ç‰‡ã€é“¾æ¥ã€å¹¿å‘Šç­‰æ— ç”¨ä¿¡æ¯
        
        Args:
            content: åŸå§‹å†…å®¹
            
        Returns:
            str: æ¸…æ´—åçš„å†…å®¹
        """
        # è®°å½•åŸå§‹é•¿åº¦
        original_length = len(content)
        logger.debug(f"[JinaSum] Original content length: {original_length}")
        
        # ç§»é™¤Markdownå›¾ç‰‡æ ‡ç­¾
        content = re.sub(r'!\[.*?\]\(.*?\)', '', content)
        content = re.sub(r'\[!\[.*?\]\(.*?\)', '', content)  # åµŒå¥—å›¾ç‰‡æ ‡ç­¾
        
        # ç§»é™¤å›¾ç‰‡æè¿° (é€šå¸¸åœ¨æ–¹æ‹¬å·æˆ–ç‰¹å®šæ ¼å¼ä¸­)
        content = re.sub(r'\[å›¾ç‰‡\]|\[image\]|\[img\]|\[picture\]', '', content, flags=re.IGNORECASE)
        content = re.sub(r'\[.*?å›¾ç‰‡.*?\]', '', content)
        
        # ç§»é™¤é˜…è¯»æ—¶é—´ã€å­—æ•°ç­‰å…ƒæ•°æ®
        content = re.sub(r'æœ¬æ–‡å­—æ•°ï¼š\d+ï¼Œé˜…è¯»æ—¶é•¿å¤§çº¦\d+åˆ†é’Ÿ', '', content)
        content = re.sub(r'é˜…è¯»æ—¶é•¿[:ï¼š].*?åˆ†é’Ÿ', '', content)
        content = re.sub(r'å­—æ•°[:ï¼š]\d+', '', content)
        
        # ç§»é™¤æ—¥æœŸæ ‡è®°å’Œæ—¶é—´æˆ³
        content = re.sub(r'\d{4}[\.å¹´/-]\d{1,2}[\.æœˆ/-]\d{1,2}[æ—¥å·]?(\s+\d{1,2}:\d{1,2}(:\d{1,2})?)?', '', content)
        
        # ç§»é™¤åˆ†éš”çº¿
        content = re.sub(r'\*\s*\*\s*\*', '', content)
        content = re.sub(r'-{3,}', '', content)
        content = re.sub(r'_{3,}', '', content)
        
        # ç§»é™¤ç½‘é¡µä¸­å¸¸è§çš„å¹¿å‘Šæ ‡è®°
        ad_patterns = [
            r'å¹¿å‘Š\s*[\.ã€‚]?', 
            r'èµåŠ©å†…å®¹', 
            r'sponsored content',
            r'advertisement',
            r'promoted content',
            r'æ¨å¹¿ä¿¡æ¯',
            r'\[å¹¿å‘Š\]',
            r'ã€å¹¿å‘Šã€‘',
        ]
        for pattern in ad_patterns:
            content = re.sub(pattern, '', content, flags=re.IGNORECASE)
        
        # ç§»é™¤URLé“¾æ¥å’Œç©ºçš„Markdowné“¾æ¥
        content = re.sub(r'https?://\S+', '', content)
        content = re.sub(r'www\.\S+', '', content)
        content = re.sub(r'\[\]\(.*?\)', '', content)  # ç©ºé“¾æ¥å¼•ç”¨ [](...)
        content = re.sub(r'\[.+?\]\(\s*\)', '', content)  # æœ‰æ–‡æœ¬æ— é“¾æ¥ [text]()
        
        # æ¸…ç†Markdownæ ¼å¼ä½†ä¿ç•™æ–‡æœ¬å†…å®¹
        content = re.sub(r'\*\*(.+?)\*\*', r'\1', content)  # ç§»é™¤åŠ ç²—æ ‡è®°ä½†ä¿ç•™å†…å®¹
        content = re.sub(r'\*(.+?)\*', r'\1', content)      # ç§»é™¤æ–œä½“æ ‡è®°ä½†ä¿ç•™å†…å®¹
        content = re.sub(r'`(.+?)`', r'\1', content)        # ç§»é™¤ä»£ç æ ‡è®°ä½†ä¿ç•™å†…å®¹
        
        # æ¸…ç†æ–‡ç« å°¾éƒ¨çš„"å¾®ä¿¡ç¼–è¾‘"å’Œ"æ¨èé˜…è¯»"ç­‰æ— å…³å†…å®¹
        content = re.sub(r'\*\*å¾®ä¿¡ç¼–è¾‘\*\*.*?$', '', content, flags=re.MULTILINE)
        content = re.sub(r'\*\*æ¨èé˜…è¯»\*\*.*?$', '', content, flags=re.MULTILINE | re.DOTALL)
        
        # æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
        content = re.sub(r'\n{3,}', '\n\n', content)  # ç§»é™¤å¤šä½™ç©ºè¡Œ
        content = re.sub(r'\s{2,}', ' ', content)     # ç§»é™¤å¤šä½™ç©ºæ ¼
        content = re.sub(r'^\s+', '', content, flags=re.MULTILINE)  # ç§»é™¤è¡Œé¦–ç©ºç™½
        content = re.sub(r'\s+$', '', content, flags=re.MULTILINE)  # ç§»é™¤è¡Œå°¾ç©ºç™½
        
        # è®°å½•æ¸…æ´—åé•¿åº¦
        cleaned_length = len(content)
        logger.debug(f"[JinaSum] Cleaned content length: {cleaned_length}, removed {original_length - cleaned_length} characters")
        
        return content